---
title: "daily-challenge"
format:
  html:
    toc: true
    toc-location: right
    toc-depth: 5
    number-sections: false
    code-fold: false
    sidebar: true
    collapse-level: 4
editor: source
editor_options: 
  chunk_output_type: console
knitr:
  opts_chunk: 
    message: false
    warning: false
    code-fold: true
    include: true
    collapse: true
    eval: true
    fig.show: hold
---

## Monday

### Life cycle of data science

In the obesity data set you detect an outlying age value of 200 and you decide to replace it with median age. Where would this fall in the life cycle of data science? (Multiple choices allowed)

-   collecting data
-   data cleaning (correct)
-   data preprocessing (correct)
-   EDA (argualbly correct)
-   variable transformations (arguably correct)
-   feature engineering

### PCA

PCA's aim is to reduce the dimensionality of the data while preserving as much variance as possible. Which of the following best describes how PCA achieves this?

-   By selecting the most important original variables based on correlation with the target outcome
-   By creating new variables (principal components) that are linear combinations of the original variables and ordered by the amount of variance they explain
-   By removing variables that are highly correlated with each other
-   By clustering the data into components and selecting the clusters with the largest number of observations

Correct: 

-   By creating new variables (principal components) that are linear combinations of the original variables and ordered by the amount of variance they explain

### Linear regression

```{r}
library(tidyverse)

data_obesity <- read_csv("data/data-obesity.csv")
data_exprs <- read_csv("data/data-obesity-genes.csv")

data <- data_obesity %>%
  select(-bp.1s, -bp.1d, -bp.2s, -bp.2d) %>%
  na.omit() %>%
  left_join(data_exprs)

model <- lm(BMI ~ LEP + age + gender, data = data)
summary(model)

```

Previous studies suggest that the expression of the LEP gene is associated with BMI.
You apply a multiple linear regression model to test this association in your dataset, while adjusting for age and gender.
Based on the regression output below, which of the following statements are correct?
(Multiple answers may be correct)

A) The association between LEP expression and BMI is statistically significant after adjusting for age and gender.
  - ✅ True – p-value for LEP is very small (2.04e-07).

B) The variable age is significantly associated with BMI in this model.
  - ❌ False – age has a high p-value (0.643), indicating no significant effect.
  
C) Male individuals tend to have a lower BMI compared to females, controlling for LEP and age.
  - ✅ True – gendermale coefficient is negative and significant.
  
D) The model explains more than half of the variability in BMI.
  - ❌ False – R-squared is only 0.13 (13%).
  
E) The coefficient for LEP means that for each unit increase in LEP expression, BMI increases by about 1.55 units, assuming age and gender are constant.
  - ✅ True – This is exactly what the coefficient represents in multiple regression.

### Logistic LASSO

You're investigating whether obesity status can be predicted from the expression levels of 10 genes of interest: FTO, RNU6-890P, SNORD115-36, MC4R, MTCO3P11, NIPSNAP1, LEP, AFF3, ZYG11A, PTPA

You decide to apply logistic regression with Lasso regularization, using:
- a train/test split,
- 10-fold cross-validation to select the best penalty parameter (lambda), and
- the final model is then fitted on the training set and evaluated on the test set.

Below is the confusion matrix and selected model coefficients:


|               | Actual No | Actual Yes |
|---------------|------------|-------------|
| Predicted No  |     40     |      2      |
| Predicted Yes |      7     |     24      |

<br>

(Intercept):  -1.95  
FTO:           1.20  
LEP:           0.88  
MC4R:         -0.44  
ZYG11A:        0.56

(All other genes have coefficient = 0)

```{r}
#| include: false
#| eval: false

# The above results are made up
# Below code is to run things if needed at some point
library(glmnet)

# data
genes_obesity <-c("FTO", "MC4R", "LEP")
data_obesity <- read_csv("data/data-obesity.csv")
data_exprs <- read_csv("data/data-obesity-genes.csv")

ft <- c(genes_obesity, colnames(data_exprs)[2:8])
ft <- sample(ft, length(ft))

data <- data_obesity %>%
  select(id, obese) %>%
  na.omit() %>%
  left_join(data_exprs) %>%
  mutate(obese = factor(obese)) %>%
  select(-id) %>%
  select("obese", all_of(ft))
  
# split data into train and test
set.seed(123)
n <- nrow(data)
test_index <- sample(seq_len(n), size = 0.2 * n)
data_test <- data[test_index, ]
data_train <- data[-test_index, ]

# Separate outcome and predictors
y_train <- data_train$obese
x_train <- data_train %>% select(-obese)

# Scale predictors
x_train_scaled <- scale(x_train)
train_means <- attr(x_train_scaled, "scaled:center")
train_sds <- attr(x_train_scaled, "scaled:scale")

# Ensure same column order as training
x_test <- data_test %>% select(-obese)
x_test <- x_test[, colnames(x_train)]

# Apply training set scaling
x_test_scaled <- scale(x_test, center = train_means, scale = train_sds)
y_test <- data_test$obese

# Fit Lasso regression with 10-fold CV
set.seed(123)
cv_model <- cv.glmnet(x_train_scaled, y_train, alpha = 1, standardize = FALSE, family = "binomial")

# Plot cross-validation error
# plot(cv_model)

# Best lambda value
best_lambda <- cv_model$lambda.1se
cat("Best lambda:", best_lambda)

# Predict obesity status give the test data
pred_test <- predict(cv_model, s = best_lambda, newx = x_test_scaled, type = "response")
print(head(pred_test))
y_pred <- ifelse(pred_test > 0.5, "Yes", "No")

# Predictions (from earlier)
pred_probs <- predict(cv_model, newx = x_test_scaled, s = best_lambda, type = "response")
pred_labels <- ifelse(pred_probs >= 0.5, 1, 0)

# confusion matrix
conf_matrix <- table(Predicted = y_pred, Actual = y_test)
print(conf_matrix)

model <- cv_model
print(coef(model, s = best_lambda))

```

A) The model correctly identifies more non-obese than obese individuals.
  - ✅ True — The confusion matrix shows 40 true negatives (No) vs 24 true positives (Yes), so more non-obese individuals were correctly classified.

B) Genes with positive coefficients increase the odds of being obese.
  - ✅ True — In logistic regression, a positive coefficient implies an increase in the log-odds (and thus odds) of the outcome (obesity = "Yes").

C) The gene MC4R is positively associated with obesity.
  - ❌ False — MC4R has a negative coefficient (-0.44), meaning higher expression is associated with lower odds of being obese.

D) Lasso has excluded some genes from the model.
  - ✅ True — Only 4 of the 10 genes have non-zero coefficients, indicating Lasso set the rest to zero (excluded them).

E) The model perfectly classifies all individuals in the test set.
  - ❌ False — The confusion matrix shows errors: 7 false positives and 2 false negatives.

F) Lasso improves interpretability by selecting only relevant genes.
  - ✅ True — Lasso’s strength is variable selection, simplifying the model by retaining only the most informative features.

### ICA

One of the main goals of Independent Component Analysis (ICA) is to uncover hidden source signals from observed mixtures. Which of the following best describes how ICA achieves this, and how it differs from PCA?

- ICA finds new variables that are linear combinations of the original features, ordered by the variance they explain, and uncorrelated with each other.
- ICA clusters the original variables into independent groups based on their mutual distance and variance.
- ICA transforms the data into a new set of components that are as statistically independent as possible, and is particularly suited for non-Gaussian data. 
- ICA standardizes the data and uses a rotation matrix to align components with maximum class separability

(correct)

- ICA transforms the data into a new set of components that are as statistically independent as possible, and is particularly suited for non-Gaussian data. 

### SOM

During the training of a Self-Organizing Map, what happens when an input vector is presented to the network?

A) All nodes in the map are updated equally to resemble the input vector
B) Only the node with the closest weight vector (Best Matching Unit) is updated to match the input vector
C) The Best Matching Unit and its neighboring nodes are adjusted to become more similar to the input vector
D) The input vector is discarded if it doesn't match any existing node exactly

Answer: C) The Best Matching Unit and its neighboring nodes are adjusted to become more similar to the input vector

Explanation:

When an input vector is introduced, the SOM identifies the Best Matching Unit (BMU)—the node whose weight vector is most similar to the input. The BMU and its neighboring nodes are then updated to more closely resemble the input vector. This neighborhood updating helps preserve the topological structure of the data on the map.

### UMAP

UMAP (Uniform Manifold Approximation and Projection) is a nonlinear dimensionality reduction technique. Which of the following best describes how UMAP constructs its low-dimensional embedding?

A) By preserving pairwise Euclidean distances between all data points in the high-dimensional space.
B) By identifying clusters in the data using k-means and projecting cluster centers to a lower-dimensional space.
C) By constructing a fuzzy topological representation of the high-dimensional data and optimizing a low-dimensional graph to preserve this structure.
D) By performing linear projections that maximize the variance captured in the first few components.

Correct Answer: C) By constructing a fuzzy topological representation of the high-dimensional data and optimizing a low-dimensional graph to preserve this structure.

Explanation:

UMAP operates by modeling the high-dimensional data as a fuzzy topological structure, capturing both local and global relationships. It then seeks a low-dimensional embedding that best preserves this structure, ensuring that the manifold's topology is maintained in the reduced space. This approach allows UMAP to effectively capture complex data patterns that linear methods might miss.

### Diffusion Maps

You are analyzing high-dimensional gene expression data and decide to apply Diffusion Maps for dimensionality reduction. After constructing the kernel matrix using a Gaussian kernel and computing the Markov transition matrix P, you raise P to the power t to simulate diffusion over time. Finally, you perform eigen decomposition on P to obtain the diffusion components.

Which of the following statements about Diffusion Maps are true? (Select all that apply)

A) The kernel matrix K measures similarity between data points, with larger values indicating greater similarity.
B) The Markov transition matrix P is obtained by normalizing K so that each row sums to 1, representing transition probabilities.
C) Raising P to the power t allows the model to capture local structures in the data more effectively.
D) Eigen decomposition of P yields eigenvectors and eigenvalues that are used to embed the data into a lower-dimensional space, capturing the main diffusion directions.
E) The first non-trivial diffusion component (DC1) often captures the global structure of the data, such as developmental trajectories.

Explanation:

A) Correct. The kernel matrix K quantifies the similarity between data points, with larger values indicating higher similarity.
B) Correct. The transition matrix P is derived by normalizing K so that each row sums to 1, converting similarities into transition probabilities.
C) Incorrect. Raising P to the power t allows the model to capture global structures by simulating diffusion over multiple steps, not just local structures.
D) Correct. Eigen decomposition of P provides eigenvectors and eigenvalues that are used to embed the data into a lower-dimensional space, highlighting the main directions of diffusion.
E) Correct. The first non-trivial diffusion component (DC1) often captures the most significant global structure in the data, such as continuous developmental processes.

## Tuesday

### Bagging

What is the main idea behind Bagging (Bootstrap Aggregating)?

- [ ] Reducing bias by focusing on misclassified examples  
- [ ] Creating a weighted average of base learners  
- [x] Reducing variance by averaging multiple models trained on bootstrapped datasets  
- [ ] Combining models sequentially to reduce training error  

---

### Random Forest

In Random Forests, which two types of randomness are introduced to decorrelate the trees?

- [ ] Different activation functions and loss functions  
- [ ] Subsampling of data and cross-validation  
- [x] Bootstrapping samples and random feature selection at each split  
- [ ] Normalization and dropout  



### Boosting vs. Bagging

What distinguishes Boosting from Bagging in ensemble learning?

- [ ] Boosting increases model variance; Bagging reduces it  
- [x] Boosting trains base learners sequentially, while Bagging trains them independently  
- [ ] Bagging uses only decision trees, while Boosting uses any model  
- [ ] Boosting works best with unsupervised learning  

---

### Stacking

Which of the following best describes Stacking in ensemble methods?

- [ ] Using bootstrapped samples to train trees  
- [ ] Averaging predictions from weak learners  
- [x] Training a meta-model to combine outputs of several base models  
- [ ] Adding more depth to individual trees  

---

### Stacking cross-validation

In the lab practical, why is cross-validation particularly important when using ensemble methods like Stacking?

- [ ] To find the best number of clusters  
- [x] To avoid overfitting when combining base and meta models  
- [ ] To balance class distributions  
- [ ] To speed up computation time  

---

### Boosting regularization

What is a typical consequence of using too many boosting iterations without regularization?

- [ ] Improved generalization to unseen data  
- [ ] Underfitting due to model simplicity  
- [ ] Increased model bias  
- [x] Overfitting to the training data  

### SVM
In a soft-margin Support Vector Machine, which of the following statements about **support vectors and the margin** is correct?

- [ ] All training points within the margin are ignored when computing the hyperplane.  
- [x] Only support vectors determine the position of the optimal hyperplane; other points have no influence.  
- [ ] Increasing the number of support vectors always increases the margin width.  
- [ ] Support vectors lie exactly on the margin boundaries in all cases, regardless of C.  

### PLS

What is a key feature of the Partial Least Squares (PLS) method in multivariate analysis?

- [ ] It maximizes the correlation between residuals and inputs  
- [ ] It decomposes the response matrix using PCA  
- [x] It finds latent components that explain both predictors and responses  
- [ ] It assumes independence between variables  

---

### PLS

In the context of PLS, what is a latent variable?

- [ ] A feature with a hidden label  
- [ ] An observed predictor not directly used in the model  
- [x] A linear combination of original predictors capturing shared variance with responses  
- [ ] A categorical variable encoded as one-hot  


## Wednesday

## Thursday

### Censoring

Below are short descriptions of different time-to-event observations from a study. Match each description to the correct type of censoring:

- A patient enters a study tracking time to cancer relapse. The study ends after 5 years, but the patient has not relapsed by then. (rigth answer: right censored)
- In a study on age at disease onset, a participant is first examined at age 40 and is already diagnosed with the condition. (right answer: left censored)
- A patient is tested for a condition every 6 months. They test negative at 6 months and positive at 12 months. (right answer: interval censored)
- A participant is enrolled in a study and attends regular follow-up visits. During one visit, they report having experienced the event of interest two months earlier. The exact date of the event is confirmed in their medical records. (right answer: no censoring)

### KM

```{r}
library(survival)
library(survminer)

# Sample survival data
set.seed(123)
data_km <- data.frame(
  time = c(6, 7, 9, 10, 13, 15, 20, 22, 25, 30),
  status = c(1, 1, 0, 1, 1, 0, 1, 1, 0, 1)  # 1 = event, 0 = censored
)

# Fit KM estimator
fit_km <- survfit(Surv(time, status) ~ 1, data = data_km)

# Plot KM curve
ggsurvplot(fit_km, data = data_km, conf.int = FALSE,
           xlab = "Time (days)", ylab = "Survival Probability",
           ggtheme = theme_minimal(), 
           color = "blue", 
           censor.size = 10)

```


Look at the KM curve. Which of the following statements are true? (Select all that apply)

A. The survival probability decreases only when an event (status = 1) occurs.
B. The plot shows 3 censored observations.
C. The Kaplan–Meier curve always ends at zero.
D. If the last observation is censored, the curve remains flat at the final value.
E. The curve accounts for censored data when estimating survival probabilities.

Answers: all but C

### Logrank

```{r}
library(survival)
library(survminer)

# Example dataset with group assignment
set.seed(42)
data_logrank <- data.frame(
  time = c(5, 6, 6, 7, 10, 12, 15, 16, 18, 22, 25, 30),
  status = c(1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1),  # 1 = event, 0 = censored
  group = c("A", "A", "A", "A", "A", "A", "B", "B", "B", "B", "B", "B")
)

# Fit survival model
fit_logrank <- survfit(Surv(time, status) ~ group, data = data_logrank)

# Log-rank test
logrank_test <- survdiff(Surv(time, status) ~ group, data = data_logrank)

fs <- 16

# Plot KM curves
ggsurvplot(fit_logrank, data = data_logrank,
           pval = FALSE, conf.int = FALSE,
           risk.table = FALSE, 
           legend.title = "Group",
           xlab = "Time", ylab = "Survival Probability",
           ggtheme = theme_minimal(), 
           censor.size = 15, 
           size = 2, 
                      font.main = c(fs, "bold"),
           font.x = c(fs),
           font.y = c(fs),
           font.tickslab = c(fs),
           font.legend = c(fs),
           font.risk.table = c(fs))

```

You observe the Kaplan–Meier curves for Group A and Group B along with a p-value from the log-rank test. Which of the following statements are true?

A. The log-rank test compares the number of observed vs. expected events in each group.
B. A small p-value suggests that the survival experience differs between the groups.
C. The log-rank test requires that the censoring occurs independently of group membership.
D. The test is valid even if the hazards in the two groups cross multiple times.
E. The log-rank test is based on the area under the survival curves.

Correct: not D, not E

### Cox PH model

```{r}
library(survival)

# Simulated data: time to event, event status, age and treatment group
set.seed(123)
data_cox <- data.frame(
  time = c(5, 6, 6, 8, 10, 12, 15, 17, 20, 22, 25, 30),
  status = c(1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1),
  age = c(60, 65, 70, 55, 50, 75, 67, 63, 58, 69, 72, 55),
  treatment = c(0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0)  # 0 = control, 1 = treated
)

# Fit Cox model with age and treatment
cox_fit <- coxph(Surv(time, status) ~ age + treatment, data = data_cox)

summary(cox_fit)

```


You fit a Cox proportional hazards model with age and treatment as covariates. Based on the summary output, which of the following statements are true?

Select all that apply:

A. Increasing age is associated with a significantly higher hazard of the event.
B. Being in the treatment group is associated with a lower estimated hazard, but the result is not statistically significant.
C. The model suggests a trend toward reduced risk in the treatment group, though not conclusive.
D. The estimated hazard ratio for age is close to 1, meaning age has little effect.
E. The model provides strong evidence that age and treatment predict time to event.

Answer: not A, not E

### Casual inference I

Q. When correlation is causation? 
C. When all the counfaders are measured and adjusted and the model assumptions are all perfectly correct. 

Q. When correlation is causation? 
A. When the correlation coefficient is statistically significant and above 0.7.
B. When two variables move together consistently across many datasets.
C. ✅ When all the confounders are measured and adjusted for, and model assumptions are perfectly correct.
D. When an experiment shows a strong effect in a small sample with a p-value < 0.05.
E. When the relationship appears consistent in observational studies and expert consensus agrees.

### Causal inference II

Q. What’s the benefit of using of causal of machine learning as opposed to using standard causal inference?
C: It will automatically take care of all the interactions and combination of confounders
C: It will take care of effect modifiers. 

❓ Quiz Question: What’s the benefit of using causal machine learning compared to standard causal inference? Mark all that apply. 

A. It eliminates the need for measuring confounders altogether.
B. It guarantees causal estimates even when the treatment is not randomized.
C. ✅ It automatically accounts for interactions and complex combinations of confounders.
D. ✅ It will take care of effect modifiers. 

### Causal inference III

How is causal inference related to missing value imputation?
A. Causal inference ignores missing values because only observed outcomes matter.
B. Causal inference assumes we know all counterfactual outcomes but just don’t use them.
C. ✅ In causal inference, we only observe one of the potential outcomes for each person, and must estimate the unobserved (counterfactual) outcome.
D. Causal inference relies only on complete cases to avoid bias from missing values.

### Casual infrence IV

❓ Quiz Question: What does heterogeneous treatment effect mean?
A. The difference between the average outcomes of the treatment and control groups.
B. The fact that treatment effects can vary across individuals or subgroups.
C. ✅ The difference in effect size between individuals or subgroups, depending on their characteristics.
D. The situation where a treatment does not affect anyone in the population.
E. A violation of randomization that makes results unpredictable.



## Friday

