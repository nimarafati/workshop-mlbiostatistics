---
title: "daily-challenge"
format:
  html:
    toc: true
    toc-location: right
    toc-depth: 5
    number-sections: false
    code-fold: false
    sidebar: true
    collapse-level: 4
editor: source
editor_options: 
  chunk_output_type: console
knitr:
  opts_chunk: 
    message: false
    warning: false
    code-fold: true
    include: true
    collapse: true
    eval: true
    fig.show: hold
---

## Monday

### Life cycle of data science

In the obesity data set you detect an outlying age value of 200 and you decide to replace it with median age. Where would this fall in the life cycle of data science? (Multiple choices allowed)

-   collecting data
-   data cleaning (correct)
-   data preprocessing (correct)
-   EDA (argualbly correct)
-   variable transformations (arguably correct)
-   feature engineering

### PCA

PCA's aim is to reduce the dimensionality of the data while preserving as much variance as possible. Which of the following best describes how PCA achieves this?

-   By selecting the most important original variables based on correlation with the target outcome
-   By creating new variables (principal components) that are linear combinations of the original variables and ordered by the amount of variance they explain
-   By removing variables that are highly correlated with each other
-   By clustering the data into components and selecting the clusters with the largest number of observations

Correct: 

-   By creating new variables (principal components) that are linear combinations of the original variables and ordered by the amount of variance they explain

### Linear regression

```{r}
library(tidyverse)

data_obesity <- read_csv("data/data-obesity.csv")
data_exprs <- read_csv("data/data-obesity-genes.csv")

data <- data_obesity %>%
  select(-bp.1s, -bp.1d, -bp.2s, -bp.2d) %>%
  na.omit() %>%
  left_join(data_exprs)

model <- lm(BMI ~ LEP + age + gender, data = data)
summary(model)

```

Previous studies suggest that the expression of the LEP gene is associated with BMI.
You apply a multiple linear regression model to test this association in your dataset, while adjusting for age and gender.
Based on the regression output below, which of the following statements are correct?
(Multiple answers may be correct)

A) The association between LEP expression and BMI is statistically significant after adjusting for age and gender.
  - ✅ True – p-value for LEP is very small (2.04e-07).

B) The variable age is significantly associated with BMI in this model.
  - ❌ False – age has a high p-value (0.643), indicating no significant effect.
  
C) Male individuals tend to have a lower BMI compared to females, controlling for LEP and age.
  - ✅ True – gendermale coefficient is negative and significant.
  
D) The model explains more than half of the variability in BMI.
  - ❌ False – R-squared is only 0.13 (13%).
  
E) The coefficient for LEP means that for each unit increase in LEP expression, BMI increases by about 1.55 units, assuming age and gender are constant.
  - ✅ True – This is exactly what the coefficient represents in multiple regression.

### Logistic LASSO

You're investigating whether obesity status can be predicted from the expression levels of 10 genes of interest: FTO, RNU6-890P, SNORD115-36, MC4R, MTCO3P11, NIPSNAP1, LEP, AFF3, ZYG11A, PTPA

You decide to apply logistic regression with Lasso regularization, using:
- a train/test split,
- 10-fold cross-validation to select the best penalty parameter (lambda), and
- the final model is then fitted on the training set and evaluated on the test set.

Below is the confusion matrix and selected model coefficients:


|               | Actual No | Actual Yes |
|---------------|------------|-------------|
| Predicted No  |     40     |      2      |
| Predicted Yes |      7     |     24      |

<br>

(Intercept):  -1.95  
FTO:           1.20  
LEP:           0.88  
MC4R:         -0.44  
ZYG11A:        0.56

(All other genes have coefficient = 0)

```{r}
#| include: false
#| eval: false

# The above results are made up
# Below code is to run things if needed at some point
library(glmnet)

# data
genes_obesity <-c("FTO", "MC4R", "LEP")
data_obesity <- read_csv("data/data-obesity.csv")
data_exprs <- read_csv("data/data-obesity-genes.csv")

ft <- c(genes_obesity, colnames(data_exprs)[2:8])
ft <- sample(ft, length(ft))

data <- data_obesity %>%
  select(id, obese) %>%
  na.omit() %>%
  left_join(data_exprs) %>%
  mutate(obese = factor(obese)) %>%
  select(-id) %>%
  select("obese", all_of(ft))
  
# split data into train and test
set.seed(123)
n <- nrow(data)
test_index <- sample(seq_len(n), size = 0.2 * n)
data_test <- data[test_index, ]
data_train <- data[-test_index, ]

# Separate outcome and predictors
y_train <- data_train$obese
x_train <- data_train %>% select(-obese)

# Scale predictors
x_train_scaled <- scale(x_train)
train_means <- attr(x_train_scaled, "scaled:center")
train_sds <- attr(x_train_scaled, "scaled:scale")

# Ensure same column order as training
x_test <- data_test %>% select(-obese)
x_test <- x_test[, colnames(x_train)]

# Apply training set scaling
x_test_scaled <- scale(x_test, center = train_means, scale = train_sds)
y_test <- data_test$obese

# Fit Lasso regression with 10-fold CV
set.seed(123)
cv_model <- cv.glmnet(x_train_scaled, y_train, alpha = 1, standardize = FALSE, family = "binomial")

# Plot cross-validation error
# plot(cv_model)

# Best lambda value
best_lambda <- cv_model$lambda.1se
cat("Best lambda:", best_lambda)

# Predict obesity status give the test data
pred_test <- predict(cv_model, s = best_lambda, newx = x_test_scaled, type = "response")
print(head(pred_test))
y_pred <- ifelse(pred_test > 0.5, "Yes", "No")

# Predictions (from earlier)
pred_probs <- predict(cv_model, newx = x_test_scaled, s = best_lambda, type = "response")
pred_labels <- ifelse(pred_probs >= 0.5, 1, 0)

# confusion matrix
conf_matrix <- table(Predicted = y_pred, Actual = y_test)
print(conf_matrix)

model <- cv_model
print(coef(model, s = best_lambda))

```

A) The model correctly identifies more non-obese than obese individuals.
  - ✅ True — The confusion matrix shows 40 true negatives (No) vs 24 true positives (Yes), so more non-obese individuals were correctly classified.

B) Genes with positive coefficients increase the odds of being obese.
  - ✅ True — In logistic regression, a positive coefficient implies an increase in the log-odds (and thus odds) of the outcome (obesity = "Yes").

C) The gene MC4R is positively associated with obesity.
  - ❌ False — MC4R has a negative coefficient (-0.44), meaning higher expression is associated with lower odds of being obese.

D) Lasso has excluded some genes from the model.
  - ✅ True — Only 4 of the 10 genes have non-zero coefficients, indicating Lasso set the rest to zero (excluded them).

E) The model perfectly classifies all individuals in the test set.
  - ❌ False — The confusion matrix shows errors: 7 false positives and 2 false negatives.

F) Lasso improves interpretability by selecting only relevant genes.
  - ✅ True — Lasso’s strength is variable selection, simplifying the model by retaining only the most informative features.

### ICA

One of the main goals of Independent Component Analysis (ICA) is to uncover hidden source signals from observed mixtures. Which of the following best describes how ICA achieves this, and how it differs from PCA?

- ICA finds new variables that are linear combinations of the original features, ordered by the variance they explain, and uncorrelated with each other.
- ICA clusters the original variables into independent groups based on their mutual distance and variance.
- ICA transforms the data into a new set of components that are as statistically independent as possible, and is particularly suited for non-Gaussian data. 
- ICA standardizes the data and uses a rotation matrix to align components with maximum class separability

(correct)

- ICA transforms the data into a new set of components that are as statistically independent as possible, and is particularly suited for non-Gaussian data. 

### SOM

During the training of a Self-Organizing Map, what happens when an input vector is presented to the network?

A) All nodes in the map are updated equally to resemble the input vector
B) Only the node with the closest weight vector (Best Matching Unit) is updated to match the input vector
C) The Best Matching Unit and its neighboring nodes are adjusted to become more similar to the input vector
D) The input vector is discarded if it doesn't match any existing node exactly

Answer: C) The Best Matching Unit and its neighboring nodes are adjusted to become more similar to the input vector

Explanation:

When an input vector is introduced, the SOM identifies the Best Matching Unit (BMU)—the node whose weight vector is most similar to the input. The BMU and its neighboring nodes are then updated to more closely resemble the input vector. This neighborhood updating helps preserve the topological structure of the data on the map.

### UMAP

UMAP (Uniform Manifold Approximation and Projection) is a nonlinear dimensionality reduction technique. Which of the following best describes how UMAP constructs its low-dimensional embedding?

A) By preserving pairwise Euclidean distances between all data points in the high-dimensional space.
B) By identifying clusters in the data using k-means and projecting cluster centers to a lower-dimensional space.
C) By constructing a fuzzy topological representation of the high-dimensional data and optimizing a low-dimensional graph to preserve this structure.
D) By performing linear projections that maximize the variance captured in the first few components.

Correct Answer: C) By constructing a fuzzy topological representation of the high-dimensional data and optimizing a low-dimensional graph to preserve this structure.

Explanation:

UMAP operates by modeling the high-dimensional data as a fuzzy topological structure, capturing both local and global relationships. It then seeks a low-dimensional embedding that best preserves this structure, ensuring that the manifold's topology is maintained in the reduced space. This approach allows UMAP to effectively capture complex data patterns that linear methods might miss.

### Diffusion Maps

You are analyzing high-dimensional gene expression data and decide to apply Diffusion Maps for dimensionality reduction. After constructing the kernel matrix using a Gaussian kernel and computing the Markov transition matrix P, you raise P to the power t to simulate diffusion over time. Finally, you perform eigen decomposition on P to obtain the diffusion components.

Which of the following statements about Diffusion Maps are true? (Select all that apply)

A) The kernel matrix K measures similarity between data points, with larger values indicating greater similarity.
B) The Markov transition matrix P is obtained by normalizing K so that each row sums to 1, representing transition probabilities.
C) Raising P to the power t allows the model to capture local structures in the data more effectively.
D) Eigen decomposition of P yields eigenvectors and eigenvalues that are used to embed the data into a lower-dimensional space, capturing the main diffusion directions.
E) The first non-trivial diffusion component (DC1) often captures the global structure of the data, such as developmental trajectories.

Explanation:

A) Correct. The kernel matrix K quantifies the similarity between data points, with larger values indicating higher similarity.
B) Correct. The transition matrix P is derived by normalizing K so that each row sums to 1, converting similarities into transition probabilities.
C) Incorrect. Raising P to the power t allows the model to capture global structures by simulating diffusion over multiple steps, not just local structures.
D) Correct. Eigen decomposition of P provides eigenvectors and eigenvalues that are used to embed the data into a lower-dimensional space, highlighting the main directions of diffusion.
E) Correct. The first non-trivial diffusion component (DC1) often captures the most significant global structure in the data, such as continuous developmental processes.

## Tuesday

### Bagging

What is the main idea behind Bagging (Bootstrap Aggregating)?

- [ ] Reducing bias by focusing on misclassified examples  
- [ ] Creating a weighted average of base learners  
- [x] Reducing variance by averaging multiple models trained on bootstrapped datasets  
- [ ] Combining models sequentially to reduce training error  

---

### Random Forest

In Random Forests, which two types of randomness are introduced to decorrelate the trees?

- [ ] Different activation functions and loss functions  
- [ ] Subsampling of data and cross-validation  
- [x] Bootstrapping samples and random feature selection at each split  
- [ ] Normalization and dropout  



### Boosting vs. Bagging

What distinguishes Boosting from Bagging in ensemble learning?

- [ ] Boosting increases model variance; Bagging reduces it  
- [x] Boosting trains base learners sequentially, while Bagging trains them independently  
- [ ] Bagging uses only decision trees, while Boosting uses any model  
- [ ] Boosting works best with unsupervised learning  

---

### Stacking

Which of the following best describes Stacking in ensemble methods?

- [ ] Using bootstrapped samples to train trees  
- [ ] Averaging predictions from weak learners  
- [x] Training a meta-model to combine outputs of several base models  
- [ ] Adding more depth to individual trees  

---

### Stacking cross-validation

In the lab practical, why is cross-validation particularly important when using ensemble methods like Stacking?

- [ ] To find the best number of clusters  
- [x] To avoid overfitting when combining base and meta models  
- [ ] To balance class distributions  
- [ ] To speed up computation time  

---

### Boosting regularization

What is a typical consequence of using too many boosting iterations without regularization?

- [ ] Improved generalization to unseen data  
- [ ] Underfitting due to model simplicity  
- [ ] Increased model bias  
- [x] Overfitting to the training data  

---

### PLS

What is a key feature of the Partial Least Squares (PLS) method in multivariate analysis?

- [ ] It maximizes the correlation between residuals and inputs  
- [ ] It decomposes the response matrix using PCA  
- [x] It finds latent components that explain both predictors and responses  
- [ ] It assumes independence between variables  

---

### PLS

In the context of PLS, what is a latent variable?

- [ ] A feature with a hidden label  
- [ ] An observed predictor not directly used in the model  
- [x] A linear combination of original predictors capturing shared variance with responses  
- [ ] A categorical variable encoded as one-hot  


## Wednesday

## Thursday

## Friday

