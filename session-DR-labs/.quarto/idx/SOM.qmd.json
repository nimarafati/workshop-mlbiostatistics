{"title":"SOM","markdown":{"yaml":{"title":"SOM","format":{"html":{"toc":true,"toc-location":"right","number-sections":true,"code-fold":false,"sidebar":true}},"editor":"visual","editor_options":{"chunk_output_type":"console"},"knitr":{"opts_chunk":{"message":false,"warning":false,"code-fold":false,"include":true,"collapse":true,"eval":true,"fig.show":"hold"}}},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n```{r}\n#| label: load-packages\n#| warning: false\n#| message: false\n\nlibrary(tidyverse)\n\n```\n\n\nThis tutorial introduces Self-Organizing Maps (SOMs), an unsupervised learning method for visualizing and clustering high-dimensional data. You’ll learn how to train a SOM, interpret its structure, and explore which features drive sample separation.\n\nBased on [https://payamemami.com/self_orginizating_maps_basics/](https://payamemami.com/self_orginizating_maps_basics/), a document that also contains more in depth information about the method.\n\n## Data\n\nWe will use a dataset, a subset from The Cancer Genome Atlas, used with the mixOmics package. It includes mRNA, miRNA, and proteomics data for 150 breast cancer training samples (Basal, Her2, Luminal A) and 70 test samples (missing proteomics). For simplicity and more samples, we combine training and test sets and focus on the miRNA data.\n\n```{r}\n#| label: load-data\n\nlibrary(mixOmics)\ndata(breast.TCGA)\n\nx <- rbind(breast.TCGA$data.train$mirna,breast.TCGA$data.test$mirna)\ngroup_labels <-c(breast.TCGA$data.train$subtype,breast.TCGA$data.test$subtype)\n\n```\n\n```{r}\n#| label: fig-data-preview\n#| fig.cap: \"Preview of the miRNA data matrix\"\n#| fig.width: 12\n#| fig.height: 10\n\n# data dimensions\nx |> dim() |> print () # dimensions of the data matrix (samples x features)\ngroup_labels |> as.factor() |> summary() # samples per group\n\n# box plots \npar(mfrow=c(2,1))\nboxplot(t(x), main=\"distribution per sample\", las=2, cex.axis=0.7, col=rainbow(10), outline=FALSE, cex.main=0.8)\nboxplot(x, main=\"distribution per miRNA\", las=2, cex.axis=0.7, col=rainbow(10), outline=FALSE, cex.main=0.8)\n\n```\n\n```{r}\n#| label: pca\n#| fig.keep: all\n\n# perform PCA\npca <- prcomp(x, center=TRUE, scale.=FALSE)\neigs <- pca$sdev^2\nvar_exp <- eigs / sum(eigs)\n\nres_pca <- data.frame(PC1=pca$x[,1], PC2=pca$x[,2], PC3=pca$x[,3], PC4=pca$x[,4], PC5=pca$x[,5]) |>\n    rownames_to_column(\"sample\") |> \n    as_tibble() \n\nres_pca_loadings <- pca$rotation\n\n# show PCA scores plots\nres_pca |>\n    ggplot(aes(x=PC1, y=PC2, color=group_labels)) +\n    geom_point() +\n    labs(title=\"PCA of miRNA data\", x=\"PC1\", y=\"PC2\") +\n    xlab(paste(\"PC1 (Var: \", round(var_exp[1] * 100, 2), \"%)\")) +\n    ylab(paste(\"PC2 (Var: \", round(var_exp[2] * 100, 2), \"%)\")) +\n    theme_minimal() +\n    scale_color_manual(values=c(\"Basal\"=\"#FF0000\", \"Her2\"=\"#00FF00\", \"LumA\"=\"#0000FF\")) +\n    theme(legend.title=element_blank())\n\n# show top 10 loadings along PC1\nres_pca_loadings |> \n    as.data.frame() |> \n    rownames_to_column(\"miRNA\") |> \n    arrange(desc(abs(PC1))) |> \n    head(10) |> \n    ggplot(aes(x=reorder(miRNA, PC1), y=PC1)) +\n    geom_bar(stat=\"identity\", fill=\"steelblue\") +\n    coord_flip() +\n    labs(title=\"Top 10 miRNAs contributing to PC1\", x=\"miRNA\", y=\"Loading\") +\n    theme_minimal()\n\n```\n\n## Train SOM\n```{r}\n#| label: som-grid\n\n# load library\nlibrary(kohonen)\n\n# define a 5x5 hexagonal grid \nsom_grid <- somgrid(xdim = 5, ydim = 5, topo = \"hexagonal\")\n# Note: you can change size depending on your data\n\n# create a fake grid\nfakesom <- list(grid = som_grid)\nclass(fakesom) <- \"kohonen\"\ndists <- unit.distances(som_grid)\nplot(fakesom, type=\"property\", property = dists[1,],\n     main=\"Distances to unit 1\", zlim=c(0,6),\n     palette = rainbow, ncolors = 7)\n\n# Note: \n# - xdim, ydim: set the grid size (e.g., 5×5)\n# - topo: defines the grid shape: \"hexagonal\" (default, smoother transitions) or \"rectangular\" (simpler, grid-like)\n# - neighbourhood.fct: determines how updates spread during training\n# - toroidal: If TRUE, wraps the grid edges (useful for cyclic data; usually FALSE in biological contexts)\n# The choice of 'topo' affects how neurons connect and how neighborhoods form during training, with hexagonal layouts often yield more natural clusters\n\n# train SOM\nset.seed(101)\n\n# scaled_data <- scale(data_input) \n# don't forget to scale your data, here we leave for demonstration purposes\n\nsom_model <- som(X = x, \n                 grid = som_grid, \n                 rlen = 1000,\n                 alpha = c(0.05, 0.01), \n                 keep.data = TRUE)\n\n# The algorithm iteratively updates neuron weights based on proximity to input data\n\n# Key parameters:\n# - data: Input data (should be scaled — SOMs are sensitive to variable ranges)\n# - grid: The SOM grid created with somgrid()\n# - rlen: Number of training iterations (how many times to cycle through the data)\n# - alpha: Learning rate (e.g., from 0.05 to 0.01, decreasing over time)\n# - keep.data: If TRUE, stores original data in the SOM object for later use\n\n# After training:\n# - Neurons (codebook vectors) are organized to reflect the structure of the data\n# - Similar data points map to nearby neurons, dissimilar ones to distant neurons\n\nsom_model |> names() |> print() \n\n# The resulting SOM object includes:\n# - unit.classif: Index of the winning neuron for each observation\n# - distances: Distance between each data point and its BMU (used to assess mapping quality)\n# - grid: Grid structure (size, topology, neighborhood)\n# - codes: Codebook vectors (final neuron weights)\n# - changes: Tracks how much weights changed each iteration (helps assess convergence)\n# - na.rows: Indices of excluded rows due to too many missing values\n# - Training parameters and layer weights (for reproducibility and interpretation)\n\n\n```\n\n## Evaluate SOM\n```{r}\n# SOMs are unsupervised so there is no loss function, but performance can still be evaluated, e.g. \n# - via quantization error\n# - mapping plot\n# - node count plot\n# - \"changes\" values\n# - U matrix\n\n# Quantization error\n# - Measures the distance between each data point and its best matching unit (BMU)\n# - Indicates how well the codebook vectors approximate the input data\n# - Lower values suggest better representation (acceptable range depends on data scale)\n\n# Calculate average quantization error across all samples (the lower the better)\nmean(som_model$distances) |> print() # average distance to BMU (quantization error)\n\n# Inspect the distribution of distances to identify poorly mapped data points\n# - if the distribution is wide or has many high values, it might indicate that the grid is too small or that the number of training iterations was insufficient.\nhist(som_model$distances, main = \"Quantization Error Distribution\", xlab = \"Distance to BMU\")\n\n# Mapping plot\nplot(som_model, type = \"mapping\")\n# Shows how input samples are distributed across the SOM grid\n# - Crowded neurons (many samples in one unit) may indicate:\n#   - Insufficient training (not enough iterations)\n#   - Learning rate too low\n#   - Grid size not appropriate (too small or too large)\n# Well-distributed samples suggest the SOM has effectively organized the data\n\n# Node count plot\nplot(som_model, type = \"count\")\n# Shows how many samples map to each neuron\n# A good SOM has most neurons occupied with some variation in density\n# Too many empty nodes → possible underfitting (short training or large grid)\n# Very few heavily occupied nodes → possible overfitting (small grid)\n\n# Changes values\nplot(som_model, type = \"changes\", main = \"Distances\")\n# Plot the changes to see how much the neuron weights change over time\n# A steadily decreasing curve means the SOM is learning\n# If it hasn’t flattened out (no plateau), consider running more iterations\n\n# U matrix (shows distances between neighboring neurons)\nplot(som_model, type = \"dis\", main = \"U-Matrix\")\n# Clear patterns (valleys/ridges) suggest meaningful clusters\n# A flat U-matrix may indicate poor learning or overly uniform data\n# If structure is unclear or quantization error is high:\n# - Try increasing iterations (rlen)\n# - Adjust learning rate (alpha), radius, or grid size\n# - Run the SOM multiple times to compare results\n\n```\n\n\n## Visualize SOM results\n```{r}\n# After training, SOMs help visualize high-dimensional data while preserving topology\n# - Similar samples are mapped to nearby neurons on the grid\n# - A mapping plot shows each sample's best matching unit (BMU)\n# - Adding group labels can reveal how well the SOM separates known classes\n# - Clear group separation → meaningful structure captured\n# - Overlap → data may not be separable or SOM needs tuning\n\ngroup_levels <- levels(group_labels)\ncolors <- setNames(c(\"tomato\", \"steelblue\",\"black\"), group_levels)  \nsample_colors <- colors[group_labels]\n\nset.seed(101)\n# Plot with colored samples\nplot(som_model, type=\"map\",\n     pchs = 21,\n     main = \"SOM Mapping by Group (Basal, Her2, Luminal A)\",\n     shape = \"straight\",\n     col = sample_colors)\nlegend(x = \"top\",legend = group_levels,col = c(\"tomato\", \"steelblue\",\"black\"),pch = 21)\n\n# The mapping plot shows a left-to-right gradient of subtypes: Luminal A → Her2 → Basal\n# - Subtypes cluster in distinct regions, not randomly mixed\n# - Suggests the SOM captured meaningful biological structure in the miRNA data\n# - It projects high-dimensional data onto a 2D grid, preserving similarity and neighborhood relationships\n# - Neurons represent prototype vectors, updated iteratively to reflect the data structure\n\n```\n\n## Cluster boundaries\n```{r}\n# Interpreting the SOM often involves identifying meaningful clusters on the grid\nplot(som_model, type=\"dist.neighbours\",\n     pchs = 21,\n     main = \"SOM distances to all immediate neighbours\",\n     shape = \"straight\")\n\n# The U-matrix (unified distance matrix) helps visualize these potential clusters:\n# - Shows average distance between each neuron and its neighbors\n# - Dark colors (red/orange) = similar neurons (low distances)\n# - Light colors (yellow/white) = dissimilar neurons (high distances)\n\n# Key observations:\n# - Uniform dark areas = consistent groups or subtypes\n# - Bright regions = natural boundaries between clusters\n\n# High-distance zones may indicate transitions between sample groups\n\n# U-matrix is useful for:\n# - Visually identifying cluster boundaries\n# - Guiding or validating later clustering (e.g., with k-means)\n# - Exploring structure in data without predefined labels\n\ncoolBlueHotRed <- function(n, alpha = 1) {\n  colors <- colorRampPalette(c(\"blue\", \"white\", \"red\"))(n)\n  adjustcolor(colors, alpha.f = alpha)\n}\n\nplot(som_model, type = \"property\", \n     property = som_model$codes[[1]][, \"hsa-mir-130b\"], \n     main = \"Expression of hsa-mir-130b\",palette.name = coolBlueHotRed)\n\n```\n\n## Property (heatmap) plot\n\n```{r}\n# Property plots (component planes) show how a single variable is distributed across the SOM grid\n\n# - Each neuron has a codebook vector (prototype) with one value per input feature\n# - The plot maps one feature (e.g., a miRNA) across all neurons using color:\n#   - Warm colors = high values, cool colors = low values\n# - Helps identify which variables are associated with specific clusters or subtypes\n# - Strong patterns suggest informative features; uniform maps suggest weak/noisy features\n# - Comparing multiple component planes can reveal co-expressed features or biomarkers\n\ncoolBlueHotRed <- function(n, alpha = 1) {\n  colors <- colorRampPalette(c(\"blue\", \"white\", \"red\"))(n)\n  adjustcolor(colors, alpha.f = alpha)\n}\n\nplot(som_model, type = \"property\", \n     property = som_model$codes[[1]][, \"hsa-mir-130b\"], \n     main = \"Expression of hsa-mir-130b\",palette.name = coolBlueHotRed)\n\nplot\n```\n\n## Clustering SOM\n```{r}\nlibrary(NbClust)\n\n# SOM organizes samples on a grid but doesn’t assign explicit cluster labels\n# - To define clusters, apply k-means (or similar) to the codebook vectors\n# - Codebook vectors represent learned patterns and are suitable for clustering\n# - we can use NBClust to help choose the optimal number of clusters:\n#   - it compares multiple validity indices to suggest the best k\n\n# Extract codebook vectors\ncodes <- som_model$codes[[1]]\n\n# Use NBClust to determine the optimal number of clusters\nset.seed(123)\nnb <- NbClust(data = codes, distance = \"euclidean\", min.nc = 2, max.nc = 10, method = \"kmeans\",index = \"gap\")\noptimal_k <- nb$Best.nc[1]\n\n# Perform k-means with the optimal number of clusters\nset.seed(123)\nkm <- kmeans(codes, centers = optimal_k, nstart = 25)\n\n# Assign a color to each cluster\ncluster_colors <- rainbow(optimal_k)[km$cluster]\n\n# Plot SOM with background colored by cluster\nplot(som_model, type = \"mapping\", \n     bgcol = cluster_colors,pch=sample_colors,shape = \"straight\",\n     main = paste(\"SOM Clustering with\", optimal_k, \"Clusters\"))\n\n# Add boundaries around clusters\nadd.cluster.boundaries(som_model, km$cluster)\n```\n\n## Exercise I (training SOM)\n\n**Task**\n\nTry out three different combinations of SOM training parameters and decide **which one best organizes the data**.\n\n- Modify:\n  - Grid size (`xdim`, `ydim`)\n  - Number of iterations (`rlen`)\n  - Learning rate (`alpha`)\n\nSuggested Configurations\n\n1. **Small grid + short training**  \n   `xdim = 4, ydim = 4, rlen = 500, alpha = c(0.05, 0.01)`\n\n2. **Medium grid + moderate training**  \n   `xdim = 6, ydim = 6, rlen = 1000, alpha = c(0.05, 0.01)`\n\n3. **Large grid + longer training**  \n   `xdim = 8, ydim = 8, rlen = 1500, alpha = c(0.1, 0.01)`\n\nFor each model, evaluate:\n\n- **Quantization error** (`mean(som_model$distances)`)\n- **Node count plot** (`plot(..., type = \"count\")`)\n- **Changes over time** (`plot(..., type = \"changes\")`)\n- **Mapping plot** to see sample distribution\n\nWhich SMO model is best for this dataset?\n\n- Which configuration gives the **lowest quantization error**?\n- Which model results in a **well-utilized grid** (few empty nodes)?\n- Does one setup show **clearer group separation** on the map?\n\n\n**Example code**\n\n```{r}\nlibrary(kohonen)\n\n# Define parameter sets\nconfigs <- list(\n  small  = list(xdim = 4, ydim = 4, rlen = 500,  alpha = c(0.05, 0.01)),\n  medium = list(xdim = 6, ydim = 6, rlen = 1000, alpha = c(0.05, 0.01)),\n  large  = list(xdim = 8, ydim = 8, rlen = 1500, alpha = c(0.1, 0.01))\n)\n\n# Train SOMs\nresults <- list()\nfor (name in names(configs)) {\n  cfg <- configs[[name]]\n  grid <- somgrid(xdim = cfg$xdim, ydim = cfg$ydim, topo = \"hexagonal\")\n  model <- som(X = x, grid = grid, rlen = cfg$rlen, alpha = cfg$alpha, keep.data = TRUE)\n  results[[name]] <- list(model = model, config = cfg)\n}\n```\n\n```{r}\n# Quantization errors\ncat(\"Quantization error:\\n\")\nsapply(results, function(r) mean(r$model$distances)) |> print()\n\n# Plot node counts\npar(mfrow = c(1, 3))\nfor (name in names(results)) {\n  plot(results[[name]]$model, type = \"count\", main = paste(name, \"Node Count\"))\n}\n\n# Plot training changes\npar(mfrow = c(1, 3))\nfor (name in names(results)) {\n  plot(results[[name]]$model, type = \"changes\", main = paste(name, \"Changes\"))\n}\n```\n\n**Interpretation**\n\nChanges Plot (Training Convergence):\n\n- All models show a clear downward trend — the SOM is learning in each case.\n- `large` has the **lowest final error** and **smoothest convergence**, indicating effective training.\n- It continues to improve throughout all 1500 iterations, with a clear plateau toward the end.\n\nNode Count Plot (Grid Utilization)\n\n- `small`: Most nodes are **heavily loaded** (some with >25 samples), suggesting **over-compression**.\n- `medium`: Shows better balance, but a few nodes still dominate.\n- `large`: **Most evenly used** — no overcrowded or empty neurons, suggesting **high resolution** and effective organization.\n\nConclusion\n\n- The `large` configuration (**8×8 grid**, **1500 iterations**, `alpha = c(0.1, 0.01)`) performs best overall — offering stable training, good resolution, and balanced use of the grid.\n\n\n## Exercise II (features)\n\n**Task**\n\n*Which Features Drive the Organization of the SOM?*\n\nIdentify which top 10 variables (miRNAs) vary most across the SOM grid. Are they the same variables that drive separation along PC1 in the PCA model? \n\nHints:\n\n- Extract the **codebook matrix** from the trained SOM using `som_model$codes[[1]]`. Each row represents a neuron, and each column a feature.\n- For each feature (e.g., miRNA), calculate the **standard deviation** across all neurons.\n- Identify the top 10 most variable features\n-  Use `plot(..., type = \"property\")` to visualize the distribution of each top feature across the grid.\n\n**Example code**\n\n```{r}\n# Extract codebook matrix\ncodes <- som_model$codes[[1]]\n\n# Calculate variability for each feature\nfeature_sd <- apply(codes, 2, sd)\n\n# Identify top 10 most variable features\ntop_features <- sort(feature_sd, decreasing = TRUE)[1:10]\nprint(top_features)\n\n# Custom color palette\ncoolBlueHotRed <- function(n, alpha = 1) {\n  colors <- colorRampPalette(c(\"blue\", \"white\", \"red\"))(n)\n  adjustcolor(colors, alpha.f = alpha)\n}\n\n# Plot top features as component planes\npar(mfrow = c(5, 2))  # grid layout for plotting\nfor (feature in names(top_features)) {\n  plot(som_model, type = \"property\",\n       property = codes[, feature],\n       main = feature,\n       palette.name = coolBlueHotRed)\n}\n\n# top features (along PC1)\ntop_loadings <- res_pca_loadings |> \n    as.data.frame() |> \n    rownames_to_column(\"miRNA\") |> \n    arrange(desc(abs(PC1))) |> \n    head(10) |> \n  dplyr::select(miRNA, PC1) \n\n# Compare the top features from SOM and PCA\nprint(top_features)\nprint(top_loadings)\nprint(intersect(names(top_features), top_loadings$miRNA))\n\n```\n\n## What to explore next\n\n*Latest advances in SOMs*\n\nSupervised variants such as generalized SOMs now incorporate label information to guide map layout for classification tasks, while supervised Kohonen networks combine clustering with prediction, useful in clinical contexts like cancer prognosis. Multi-omics and multi-view extensions, including layered SOMs, allow integration of diverse data types (e.g., mRNA, miRNA, proteomics) by aligning SOM layers through shared topology. Time-aware models like SOMTimeS apply dynamic time warping to cluster biological time series such as longitudinal gene expression. In single-cell biology, FlowSOM efficiently handles millions of cytometry events and is widely adopted in platforms like Bioconductor. Visualization has improved with tools like U-Matrix++ and projection-based SOMs that embed t-SNE or UMAP to retain global data structure. Finally, scalable implementations using TensorFlow, PyTorch, or Julia have enabled SOMs to process large-scale omics and clinical datasets using GPU acceleration\n\n\n","srcMarkdownNoYaml":"\n\n```{r}\n#| label: load-packages\n#| warning: false\n#| message: false\n\nlibrary(tidyverse)\n\n```\n\n## Introduction\n\nThis tutorial introduces Self-Organizing Maps (SOMs), an unsupervised learning method for visualizing and clustering high-dimensional data. You’ll learn how to train a SOM, interpret its structure, and explore which features drive sample separation.\n\nBased on [https://payamemami.com/self_orginizating_maps_basics/](https://payamemami.com/self_orginizating_maps_basics/), a document that also contains more in depth information about the method.\n\n## Data\n\nWe will use a dataset, a subset from The Cancer Genome Atlas, used with the mixOmics package. It includes mRNA, miRNA, and proteomics data for 150 breast cancer training samples (Basal, Her2, Luminal A) and 70 test samples (missing proteomics). For simplicity and more samples, we combine training and test sets and focus on the miRNA data.\n\n```{r}\n#| label: load-data\n\nlibrary(mixOmics)\ndata(breast.TCGA)\n\nx <- rbind(breast.TCGA$data.train$mirna,breast.TCGA$data.test$mirna)\ngroup_labels <-c(breast.TCGA$data.train$subtype,breast.TCGA$data.test$subtype)\n\n```\n\n```{r}\n#| label: fig-data-preview\n#| fig.cap: \"Preview of the miRNA data matrix\"\n#| fig.width: 12\n#| fig.height: 10\n\n# data dimensions\nx |> dim() |> print () # dimensions of the data matrix (samples x features)\ngroup_labels |> as.factor() |> summary() # samples per group\n\n# box plots \npar(mfrow=c(2,1))\nboxplot(t(x), main=\"distribution per sample\", las=2, cex.axis=0.7, col=rainbow(10), outline=FALSE, cex.main=0.8)\nboxplot(x, main=\"distribution per miRNA\", las=2, cex.axis=0.7, col=rainbow(10), outline=FALSE, cex.main=0.8)\n\n```\n\n```{r}\n#| label: pca\n#| fig.keep: all\n\n# perform PCA\npca <- prcomp(x, center=TRUE, scale.=FALSE)\neigs <- pca$sdev^2\nvar_exp <- eigs / sum(eigs)\n\nres_pca <- data.frame(PC1=pca$x[,1], PC2=pca$x[,2], PC3=pca$x[,3], PC4=pca$x[,4], PC5=pca$x[,5]) |>\n    rownames_to_column(\"sample\") |> \n    as_tibble() \n\nres_pca_loadings <- pca$rotation\n\n# show PCA scores plots\nres_pca |>\n    ggplot(aes(x=PC1, y=PC2, color=group_labels)) +\n    geom_point() +\n    labs(title=\"PCA of miRNA data\", x=\"PC1\", y=\"PC2\") +\n    xlab(paste(\"PC1 (Var: \", round(var_exp[1] * 100, 2), \"%)\")) +\n    ylab(paste(\"PC2 (Var: \", round(var_exp[2] * 100, 2), \"%)\")) +\n    theme_minimal() +\n    scale_color_manual(values=c(\"Basal\"=\"#FF0000\", \"Her2\"=\"#00FF00\", \"LumA\"=\"#0000FF\")) +\n    theme(legend.title=element_blank())\n\n# show top 10 loadings along PC1\nres_pca_loadings |> \n    as.data.frame() |> \n    rownames_to_column(\"miRNA\") |> \n    arrange(desc(abs(PC1))) |> \n    head(10) |> \n    ggplot(aes(x=reorder(miRNA, PC1), y=PC1)) +\n    geom_bar(stat=\"identity\", fill=\"steelblue\") +\n    coord_flip() +\n    labs(title=\"Top 10 miRNAs contributing to PC1\", x=\"miRNA\", y=\"Loading\") +\n    theme_minimal()\n\n```\n\n## Train SOM\n```{r}\n#| label: som-grid\n\n# load library\nlibrary(kohonen)\n\n# define a 5x5 hexagonal grid \nsom_grid <- somgrid(xdim = 5, ydim = 5, topo = \"hexagonal\")\n# Note: you can change size depending on your data\n\n# create a fake grid\nfakesom <- list(grid = som_grid)\nclass(fakesom) <- \"kohonen\"\ndists <- unit.distances(som_grid)\nplot(fakesom, type=\"property\", property = dists[1,],\n     main=\"Distances to unit 1\", zlim=c(0,6),\n     palette = rainbow, ncolors = 7)\n\n# Note: \n# - xdim, ydim: set the grid size (e.g., 5×5)\n# - topo: defines the grid shape: \"hexagonal\" (default, smoother transitions) or \"rectangular\" (simpler, grid-like)\n# - neighbourhood.fct: determines how updates spread during training\n# - toroidal: If TRUE, wraps the grid edges (useful for cyclic data; usually FALSE in biological contexts)\n# The choice of 'topo' affects how neurons connect and how neighborhoods form during training, with hexagonal layouts often yield more natural clusters\n\n# train SOM\nset.seed(101)\n\n# scaled_data <- scale(data_input) \n# don't forget to scale your data, here we leave for demonstration purposes\n\nsom_model <- som(X = x, \n                 grid = som_grid, \n                 rlen = 1000,\n                 alpha = c(0.05, 0.01), \n                 keep.data = TRUE)\n\n# The algorithm iteratively updates neuron weights based on proximity to input data\n\n# Key parameters:\n# - data: Input data (should be scaled — SOMs are sensitive to variable ranges)\n# - grid: The SOM grid created with somgrid()\n# - rlen: Number of training iterations (how many times to cycle through the data)\n# - alpha: Learning rate (e.g., from 0.05 to 0.01, decreasing over time)\n# - keep.data: If TRUE, stores original data in the SOM object for later use\n\n# After training:\n# - Neurons (codebook vectors) are organized to reflect the structure of the data\n# - Similar data points map to nearby neurons, dissimilar ones to distant neurons\n\nsom_model |> names() |> print() \n\n# The resulting SOM object includes:\n# - unit.classif: Index of the winning neuron for each observation\n# - distances: Distance between each data point and its BMU (used to assess mapping quality)\n# - grid: Grid structure (size, topology, neighborhood)\n# - codes: Codebook vectors (final neuron weights)\n# - changes: Tracks how much weights changed each iteration (helps assess convergence)\n# - na.rows: Indices of excluded rows due to too many missing values\n# - Training parameters and layer weights (for reproducibility and interpretation)\n\n\n```\n\n## Evaluate SOM\n```{r}\n# SOMs are unsupervised so there is no loss function, but performance can still be evaluated, e.g. \n# - via quantization error\n# - mapping plot\n# - node count plot\n# - \"changes\" values\n# - U matrix\n\n# Quantization error\n# - Measures the distance between each data point and its best matching unit (BMU)\n# - Indicates how well the codebook vectors approximate the input data\n# - Lower values suggest better representation (acceptable range depends on data scale)\n\n# Calculate average quantization error across all samples (the lower the better)\nmean(som_model$distances) |> print() # average distance to BMU (quantization error)\n\n# Inspect the distribution of distances to identify poorly mapped data points\n# - if the distribution is wide or has many high values, it might indicate that the grid is too small or that the number of training iterations was insufficient.\nhist(som_model$distances, main = \"Quantization Error Distribution\", xlab = \"Distance to BMU\")\n\n# Mapping plot\nplot(som_model, type = \"mapping\")\n# Shows how input samples are distributed across the SOM grid\n# - Crowded neurons (many samples in one unit) may indicate:\n#   - Insufficient training (not enough iterations)\n#   - Learning rate too low\n#   - Grid size not appropriate (too small or too large)\n# Well-distributed samples suggest the SOM has effectively organized the data\n\n# Node count plot\nplot(som_model, type = \"count\")\n# Shows how many samples map to each neuron\n# A good SOM has most neurons occupied with some variation in density\n# Too many empty nodes → possible underfitting (short training or large grid)\n# Very few heavily occupied nodes → possible overfitting (small grid)\n\n# Changes values\nplot(som_model, type = \"changes\", main = \"Distances\")\n# Plot the changes to see how much the neuron weights change over time\n# A steadily decreasing curve means the SOM is learning\n# If it hasn’t flattened out (no plateau), consider running more iterations\n\n# U matrix (shows distances between neighboring neurons)\nplot(som_model, type = \"dis\", main = \"U-Matrix\")\n# Clear patterns (valleys/ridges) suggest meaningful clusters\n# A flat U-matrix may indicate poor learning or overly uniform data\n# If structure is unclear or quantization error is high:\n# - Try increasing iterations (rlen)\n# - Adjust learning rate (alpha), radius, or grid size\n# - Run the SOM multiple times to compare results\n\n```\n\n\n## Visualize SOM results\n```{r}\n# After training, SOMs help visualize high-dimensional data while preserving topology\n# - Similar samples are mapped to nearby neurons on the grid\n# - A mapping plot shows each sample's best matching unit (BMU)\n# - Adding group labels can reveal how well the SOM separates known classes\n# - Clear group separation → meaningful structure captured\n# - Overlap → data may not be separable or SOM needs tuning\n\ngroup_levels <- levels(group_labels)\ncolors <- setNames(c(\"tomato\", \"steelblue\",\"black\"), group_levels)  \nsample_colors <- colors[group_labels]\n\nset.seed(101)\n# Plot with colored samples\nplot(som_model, type=\"map\",\n     pchs = 21,\n     main = \"SOM Mapping by Group (Basal, Her2, Luminal A)\",\n     shape = \"straight\",\n     col = sample_colors)\nlegend(x = \"top\",legend = group_levels,col = c(\"tomato\", \"steelblue\",\"black\"),pch = 21)\n\n# The mapping plot shows a left-to-right gradient of subtypes: Luminal A → Her2 → Basal\n# - Subtypes cluster in distinct regions, not randomly mixed\n# - Suggests the SOM captured meaningful biological structure in the miRNA data\n# - It projects high-dimensional data onto a 2D grid, preserving similarity and neighborhood relationships\n# - Neurons represent prototype vectors, updated iteratively to reflect the data structure\n\n```\n\n## Cluster boundaries\n```{r}\n# Interpreting the SOM often involves identifying meaningful clusters on the grid\nplot(som_model, type=\"dist.neighbours\",\n     pchs = 21,\n     main = \"SOM distances to all immediate neighbours\",\n     shape = \"straight\")\n\n# The U-matrix (unified distance matrix) helps visualize these potential clusters:\n# - Shows average distance between each neuron and its neighbors\n# - Dark colors (red/orange) = similar neurons (low distances)\n# - Light colors (yellow/white) = dissimilar neurons (high distances)\n\n# Key observations:\n# - Uniform dark areas = consistent groups or subtypes\n# - Bright regions = natural boundaries between clusters\n\n# High-distance zones may indicate transitions between sample groups\n\n# U-matrix is useful for:\n# - Visually identifying cluster boundaries\n# - Guiding or validating later clustering (e.g., with k-means)\n# - Exploring structure in data without predefined labels\n\ncoolBlueHotRed <- function(n, alpha = 1) {\n  colors <- colorRampPalette(c(\"blue\", \"white\", \"red\"))(n)\n  adjustcolor(colors, alpha.f = alpha)\n}\n\nplot(som_model, type = \"property\", \n     property = som_model$codes[[1]][, \"hsa-mir-130b\"], \n     main = \"Expression of hsa-mir-130b\",palette.name = coolBlueHotRed)\n\n```\n\n## Property (heatmap) plot\n\n```{r}\n# Property plots (component planes) show how a single variable is distributed across the SOM grid\n\n# - Each neuron has a codebook vector (prototype) with one value per input feature\n# - The plot maps one feature (e.g., a miRNA) across all neurons using color:\n#   - Warm colors = high values, cool colors = low values\n# - Helps identify which variables are associated with specific clusters or subtypes\n# - Strong patterns suggest informative features; uniform maps suggest weak/noisy features\n# - Comparing multiple component planes can reveal co-expressed features or biomarkers\n\ncoolBlueHotRed <- function(n, alpha = 1) {\n  colors <- colorRampPalette(c(\"blue\", \"white\", \"red\"))(n)\n  adjustcolor(colors, alpha.f = alpha)\n}\n\nplot(som_model, type = \"property\", \n     property = som_model$codes[[1]][, \"hsa-mir-130b\"], \n     main = \"Expression of hsa-mir-130b\",palette.name = coolBlueHotRed)\n\nplot\n```\n\n## Clustering SOM\n```{r}\nlibrary(NbClust)\n\n# SOM organizes samples on a grid but doesn’t assign explicit cluster labels\n# - To define clusters, apply k-means (or similar) to the codebook vectors\n# - Codebook vectors represent learned patterns and are suitable for clustering\n# - we can use NBClust to help choose the optimal number of clusters:\n#   - it compares multiple validity indices to suggest the best k\n\n# Extract codebook vectors\ncodes <- som_model$codes[[1]]\n\n# Use NBClust to determine the optimal number of clusters\nset.seed(123)\nnb <- NbClust(data = codes, distance = \"euclidean\", min.nc = 2, max.nc = 10, method = \"kmeans\",index = \"gap\")\noptimal_k <- nb$Best.nc[1]\n\n# Perform k-means with the optimal number of clusters\nset.seed(123)\nkm <- kmeans(codes, centers = optimal_k, nstart = 25)\n\n# Assign a color to each cluster\ncluster_colors <- rainbow(optimal_k)[km$cluster]\n\n# Plot SOM with background colored by cluster\nplot(som_model, type = \"mapping\", \n     bgcol = cluster_colors,pch=sample_colors,shape = \"straight\",\n     main = paste(\"SOM Clustering with\", optimal_k, \"Clusters\"))\n\n# Add boundaries around clusters\nadd.cluster.boundaries(som_model, km$cluster)\n```\n\n## Exercise I (training SOM)\n\n**Task**\n\nTry out three different combinations of SOM training parameters and decide **which one best organizes the data**.\n\n- Modify:\n  - Grid size (`xdim`, `ydim`)\n  - Number of iterations (`rlen`)\n  - Learning rate (`alpha`)\n\nSuggested Configurations\n\n1. **Small grid + short training**  \n   `xdim = 4, ydim = 4, rlen = 500, alpha = c(0.05, 0.01)`\n\n2. **Medium grid + moderate training**  \n   `xdim = 6, ydim = 6, rlen = 1000, alpha = c(0.05, 0.01)`\n\n3. **Large grid + longer training**  \n   `xdim = 8, ydim = 8, rlen = 1500, alpha = c(0.1, 0.01)`\n\nFor each model, evaluate:\n\n- **Quantization error** (`mean(som_model$distances)`)\n- **Node count plot** (`plot(..., type = \"count\")`)\n- **Changes over time** (`plot(..., type = \"changes\")`)\n- **Mapping plot** to see sample distribution\n\nWhich SMO model is best for this dataset?\n\n- Which configuration gives the **lowest quantization error**?\n- Which model results in a **well-utilized grid** (few empty nodes)?\n- Does one setup show **clearer group separation** on the map?\n\n\n**Example code**\n\n```{r}\nlibrary(kohonen)\n\n# Define parameter sets\nconfigs <- list(\n  small  = list(xdim = 4, ydim = 4, rlen = 500,  alpha = c(0.05, 0.01)),\n  medium = list(xdim = 6, ydim = 6, rlen = 1000, alpha = c(0.05, 0.01)),\n  large  = list(xdim = 8, ydim = 8, rlen = 1500, alpha = c(0.1, 0.01))\n)\n\n# Train SOMs\nresults <- list()\nfor (name in names(configs)) {\n  cfg <- configs[[name]]\n  grid <- somgrid(xdim = cfg$xdim, ydim = cfg$ydim, topo = \"hexagonal\")\n  model <- som(X = x, grid = grid, rlen = cfg$rlen, alpha = cfg$alpha, keep.data = TRUE)\n  results[[name]] <- list(model = model, config = cfg)\n}\n```\n\n```{r}\n# Quantization errors\ncat(\"Quantization error:\\n\")\nsapply(results, function(r) mean(r$model$distances)) |> print()\n\n# Plot node counts\npar(mfrow = c(1, 3))\nfor (name in names(results)) {\n  plot(results[[name]]$model, type = \"count\", main = paste(name, \"Node Count\"))\n}\n\n# Plot training changes\npar(mfrow = c(1, 3))\nfor (name in names(results)) {\n  plot(results[[name]]$model, type = \"changes\", main = paste(name, \"Changes\"))\n}\n```\n\n**Interpretation**\n\nChanges Plot (Training Convergence):\n\n- All models show a clear downward trend — the SOM is learning in each case.\n- `large` has the **lowest final error** and **smoothest convergence**, indicating effective training.\n- It continues to improve throughout all 1500 iterations, with a clear plateau toward the end.\n\nNode Count Plot (Grid Utilization)\n\n- `small`: Most nodes are **heavily loaded** (some with >25 samples), suggesting **over-compression**.\n- `medium`: Shows better balance, but a few nodes still dominate.\n- `large`: **Most evenly used** — no overcrowded or empty neurons, suggesting **high resolution** and effective organization.\n\nConclusion\n\n- The `large` configuration (**8×8 grid**, **1500 iterations**, `alpha = c(0.1, 0.01)`) performs best overall — offering stable training, good resolution, and balanced use of the grid.\n\n\n## Exercise II (features)\n\n**Task**\n\n*Which Features Drive the Organization of the SOM?*\n\nIdentify which top 10 variables (miRNAs) vary most across the SOM grid. Are they the same variables that drive separation along PC1 in the PCA model? \n\nHints:\n\n- Extract the **codebook matrix** from the trained SOM using `som_model$codes[[1]]`. Each row represents a neuron, and each column a feature.\n- For each feature (e.g., miRNA), calculate the **standard deviation** across all neurons.\n- Identify the top 10 most variable features\n-  Use `plot(..., type = \"property\")` to visualize the distribution of each top feature across the grid.\n\n**Example code**\n\n```{r}\n# Extract codebook matrix\ncodes <- som_model$codes[[1]]\n\n# Calculate variability for each feature\nfeature_sd <- apply(codes, 2, sd)\n\n# Identify top 10 most variable features\ntop_features <- sort(feature_sd, decreasing = TRUE)[1:10]\nprint(top_features)\n\n# Custom color palette\ncoolBlueHotRed <- function(n, alpha = 1) {\n  colors <- colorRampPalette(c(\"blue\", \"white\", \"red\"))(n)\n  adjustcolor(colors, alpha.f = alpha)\n}\n\n# Plot top features as component planes\npar(mfrow = c(5, 2))  # grid layout for plotting\nfor (feature in names(top_features)) {\n  plot(som_model, type = \"property\",\n       property = codes[, feature],\n       main = feature,\n       palette.name = coolBlueHotRed)\n}\n\n# top features (along PC1)\ntop_loadings <- res_pca_loadings |> \n    as.data.frame() |> \n    rownames_to_column(\"miRNA\") |> \n    arrange(desc(abs(PC1))) |> \n    head(10) |> \n  dplyr::select(miRNA, PC1) \n\n# Compare the top features from SOM and PCA\nprint(top_features)\nprint(top_loadings)\nprint(intersect(names(top_features), top_loadings$miRNA))\n\n```\n\n## What to explore next\n\n*Latest advances in SOMs*\n\nSupervised variants such as generalized SOMs now incorporate label information to guide map layout for classification tasks, while supervised Kohonen networks combine clustering with prediction, useful in clinical contexts like cancer prognosis. Multi-omics and multi-view extensions, including layered SOMs, allow integration of diverse data types (e.g., mRNA, miRNA, proteomics) by aligning SOM layers through shared topology. Time-aware models like SOMTimeS apply dynamic time warping to cluster biological time series such as longitudinal gene expression. In single-cell biology, FlowSOM efficiently handles millions of cytometry events and is widely adopted in platforms like Bioconductor. Visualization has improved with tools like U-Matrix++ and projection-based SOMs that embed t-SNE or UMAP to retain global data structure. Finally, scalable implementations using TensorFlow, PyTorch, or Julia have enabled SOMs to process large-scale omics and clinical datasets using GPU acceleration\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"number-sections":true,"output-file":"SOM.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.36","bibliography":["references.bib"],"theme":"spacebar","mermaid":{"theme":"forest"},"title":"SOM","editor":"visual","editor_options":{"chunk_output_type":"console"},"knitr":{"opts_chunk":{"message":false,"warning":false,"code-fold":false,"include":true,"collapse":true,"eval":true,"fig.show":"hold"}},"toc-location":"right","sidebar":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}