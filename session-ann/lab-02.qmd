---
title: "Bagging, boosting, stacking"
format:
  html:
    toc: true
    toc-location: right
    toc-depth: 5
    number-sections: false
    code-fold: false
    sidebar: true
    collapse-level: 4
editor: source
editor_options: 
  chunk_output_type: console
knitr:
  opts_chunk: 
    message: false
    warning: false
    code-fold: false
    include: true
    collapse: true
    eval: true
    fig.show: hold
---

## Introduction

We are still working to better understand obesity. Previous studies have shown that the expression of several genes FTO, MC4R, LEP, LEPR, POMC, and PCSK1 is associated with the condition. However, it remains unclear whether these are the only genes involved. We are thus interested in exploring whether a predictive model can be built using less well-known genes along with clinical data.

We will revisit Random Forest as an example of a bagging approach. Next, we will apply boosting using XGBoost. Finally, we will combine the predictions from multiple models using a stacking ensemble.

## Load packages

```{r}
#| label: load-packages

# load packages
rm(list=ls())
library(tidyverse)
library(ggcorrplot)
library(glmnet) # for fitting GLMs
library(fastDummies) # for features processing
library(pROC)      # for ROC curves and AUC
library(ranger) # for random forest
library(ranger)
library(e1071)
library(class)
library(xgboost)


```

## Load Data

```{r}
#| label: load-data

# known obesity genes
obesity_genes <- c("FTO", "MC4R", "LEP", "LEPR", "POMC", "PCSK1")

# Load the clinical data
data_obesity <- read_csv("data/data-obesity.csv") 

# Load gene expression data
data_expr <- read_csv("data/data-obesity-genes.csv")

# preview clinical data
dim(data_obesity)
glimpse(data_obesity)

# Based on the number of missing data, let's delete bp.2s, bp.2d
# and use complete-cases analysis 
data_obesity <- data_obesity %>%
  dplyr::select(-bp.2s, -bp.2d) %>%
  na.omit()

```

## Split Data

```{r}
#| label: split-data
#| eval: true
#| warning: false
#| message: false
#| code-fold: false

# join gene expression data with clinical data
data <- data_obesity %>%
  dplyr::select(-BMI) %>%
  left_join(data_expr, by = "id") %>%
  dplyr::select(-obesity_genes)

# split data into train and test
set.seed(123)
n <- nrow(data)
test_index <- sample(seq_len(n), size = 0.2 * n)
data_test <- data[test_index, ]
data_train <- data[-test_index, ]
```

## Feature Engineering and Scaling

```{r}
#| label: feature-eng
#| eval: true
#| warning: false
#| message: false
#| code-fold: false

# Conversion factors
inch2m <- 2.54 / 100
pound2kg <- 0.45

# ---- Process Training Data ----
data_train_processed <- data_train %>%
  mutate(
    height = round(height * inch2m, 2),
    weight = round(weight * pound2kg, 2),
    glu = log(stab.glu)
  ) %>%
  select(-stab.glu, -id)

# Remove zero-variance features
nzv <- sapply(data_train_processed, function(x) length(unique(x)) > 1)
data_train_processed <- data_train_processed[, nzv]

# Remove highly correlated predictors (|r| > 0.8)
cor_matrix <- cor(select(data_train_processed, where(is.numeric)))
high_corr <- names(which(apply(cor_matrix, 2, function(x) any(abs(x) > 0.8 & abs(x) < 1))))

data_train_processed <- data_train_processed %>% select(-c("weight", "waist", "hip"))

# Dummy encode categorical variables
data_train_processed <- dummy_cols(data_train_processed,
                                   select_columns = c("location", "gender", "frame"),
                                   remove_selected_columns = TRUE)

# Separate outcome and predictors
data_train_processed <- data_train %>%
  select("obese")

y_train <- data_train_processed$obese
x_train <- data_train_processed %>% select(-obese)

# Scale predictors
x_train_scaled <- scale(x_train)
train_means <- attr(x_train_scaled, "scaled:center")
train_sds <- attr(x_train_scaled, "scaled:scale")
```

## Prepare Test Data with Same Processing

```{r}
#| label: process-test-data
#| eval: true
#| warning: false
#| message: false
#| code-fold: false

# ---- Process Test Data ----
data_test_processed <- data_test %>%
  mutate(
    height = round(height * inch2m, 2),
    weight = round(weight * pound2kg, 2),
    glu = log(stab.glu)
  ) %>%
  select(-stab.glu, -id)

# Dummy encode categorical variables
data_test_processed <- dummy_cols(data_test_processed,
                                  select_columns = c("location", "gender", "frame"),
                                  remove_selected_columns = TRUE)

# Remove same columns as in training
data_test_processed <- data_test_processed %>%
  select(colnames(data_train_processed))

# Ensure same column order as training
x_test <- data_test_processed %>% select(-obese)
x_test <- x_test[, colnames(x_train)]

# Apply training set scaling
x_test_scaled <- scale(x_test, center = train_means, scale = train_sds)
y_test <- data_test_processed$obese

```

## Bagging (Random Forest)

```{r}
#| label: bagging-rf-tune

# Combine predictors and target into a single data frame for ranger
train_df <- data.frame(y = factor(y_train), x_train_scaled)
test_df <- data.frame(x_test_scaled)

# Tune RF
# Grid search over mtry and min.node.size
results <- expand.grid(
  mtry = c(5, 10, 20, 50),
  min.node.size = c(1, 5, 10)
)

# Create a function to evaluate AUC for a set of parameters
tune_rf <- function(mtry_val, min_node) {
  rf_model <- ranger(
    y ~ ., data = train_df,
    probability = TRUE,
    num.trees = 500,
    mtry = mtry_val,
    min.node.size = min_node,
    seed = 123
  )
  
  rf_probs <- predict(rf_model, data = test_df)$predictions[, "Yes"]
  roc_obj <- roc(y_test, rf_probs, quiet = TRUE)
  auc_val <- auc(roc_obj)
  return(auc_val)
}

results$AUC <- mapply(tune_rf, results$mtry, results$min.node.size)

# Find best combination
best_params <- results[which.max(results$AUC), ]
print(best_params)

# Refit final tuned Random Forest model with best hyperparameters
rf_model <- ranger(
  y ~ ., data = train_df,
  probability = TRUE,
  num.trees = 500,
  mtry = best_params$mtry,
  min.node.size = best_params$min.node.size,
  seed = 123, 
  importance = "impurity"
)

# Predict and evaluate
rf_probs <- predict(rf_model, data = test_df)$predictions[, "Yes"]
rf_preds <- ifelse(rf_probs > 0.5, "Yes", "No")

# Confusion matrix
conf_matrix <- table(Predicted = rf_preds, Actual = y_test)
print(conf_matrix)

# Accuracy and AUC
rf_acc <- sum(diag(conf_matrix)) / sum(conf_matrix)
roc_obj <- roc(y_test, rf_probs)
rf_auc_val <- auc(roc_obj)

cat("Tuned RF Accuracy:", round(rf_acc, 3), "\n")
cat("Tuned RF AUC:", round(rf_auc_val, 3), "\n")

```


```{r}
#| label: rf-vip

# feature importance
importance <- rf_model$variable.importance

# Convert to data frame
importance_df <- data.frame(
  feature = names(importance),
  importance = importance
)

# Plot top features
importance_df <- importance_df %>%
  arrange(desc(importance))

ggplot(importance_df[1:20, ], aes(x = reorder(feature, importance), y = importance)) +
  geom_col(fill = "darkgreen") +
  coord_flip() +
  labs(
    title = "Variable Importance (Random Forest)",
    x = "Feature", y = "Importance (Gini Impurity)"
  ) +
  theme_minimal()

```

## Boosting 

## Stacking


## Lasso Regression with Cross-Validation


```{r}
#| label: fit-lassso
#| eval: true
#| warning: false
#| message: false
#| code-fold: false

# Fit Lasso regression with 10-fold CV
set.seed(123)
cv_model <- cv.glmnet(x_train_scaled, y_train, alpha = 1, standardize = FALSE, family = "binomial")

# Plot cross-validation error
plot(cv_model)

# Best lambda value
best_lambda <- cv_model$lambda.1se
cat("Best lambda:", best_lambda)
```

### Evaluate Model on Test Data

```{r}
#| label: evaluate-model
#| eval: true
#| warning: false
#| message: false
#| code-fold: false

# Predict obesity status give the test data
pred_test <- predict(cv_model, s = best_lambda, newx = x_test_scaled, type = "response")
print(head(pred_test))
y_pred <- ifelse(pred_test > 0.5, "Yes", "No")

# Predictions (from earlier)
pred_probs <- predict(cv_model, newx = x_test_scaled, s = best_lambda, type = "response")
pred_labels <- ifelse(pred_probs >= 0.5, 1, 0)

# confusion matrix
conf_matrix <- table(Predicted = y_pred, Actual = y_test)
print(conf_matrix)

# accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Accuracy:", round(accuracy, 3), "\n")

# compute precision, recall, F1 
TP <- conf_matrix["Yes", "Yes"]
FP <- conf_matrix["Yes", "No"]
FN <- conf_matrix["No", "Yes"]

precision <- TP / (TP + FP)
recall    <- TP / (TP + FN)
f1_score  <- 2 * (precision * recall) / (precision + recall)

cat("Precision:", round(precision, 3), "\n")
cat("Recall:", round(recall, 3), "\n")
cat("F1 Score:", round(f1_score, 3), "\n")

# ROC Curve and AUC
y_test_numeric <- ifelse(y_test == "Yes", 1, 0)  # Convert to numeric for ROC
y_pred_numeric <- as.numeric(ifelse(y_pred == "Yes", 1, 0))  # Convert to numeric for ROC

roc_obj <- roc(y_test_numeric, as.numeric(pred_probs))
plot(roc_obj, main = "ROC Curve", col = "blue", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "gray")

auc_val <- auc(roc_obj)
cat("AUC:", round(auc_val, 3), "\n")

```

### Variable Importance Plot

```{r}
#| label: vip
#| eval: true
#| warning: false
#| message: false
#| code-fold: false

# Extract coefficients
coef_matrix <- coef(cv_model, s = best_lambda)
coef_df <- as.data.frame(as.matrix(coef_matrix))
coef_df$feature <- rownames(coef_df)
colnames(coef_df)[1] <- "coefficient"

# Filter out intercept and zero coefficients
coef_df <- coef_df %>%
  filter(feature != "(Intercept)", coefficient != 0) %>%
  mutate(abs_coef = abs(coefficient)) %>%
  arrange(desc(abs_coef))

# Plot
ggplot(coef_df, aes(x = reorder(feature, abs_coef), y = abs_coef)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Variable Importance (Lasso Coefficients)",
       x = "Feature", y = "Absolute Coefficient") +
  theme_minimal()
```




## ChatGPT tutorial

```{r}
### Ensemble Learning in Base R: LASSO, Random Forest, SVM, KNN + Stacking, Bagging, Boosting

# Load required libraries
library(glmnet)
library(ranger)
library(e1071)
library(class)
library(dplyr)
library(xgboost)

# Load and clean data
data <- read.csv("your_data.csv")

# Remove columns with too many missing values
missing_threshold <- 0.3  # 30% missing allowed
missing_frac <- colMeans(is.na(data))
data <- data[, missing_frac <= missing_threshold]

# Drop remaining rows with missing data
data <- na.omit(data)

# Convert target to factor
data$obese <- factor(data$obese, levels = c("No", "Yes"))

# Train/test split
set.seed(123)
n <- nrow(data)
train_idx <- sample(seq_len(n), size = 0.8 * n)
train_data <- data[train_idx, ]
test_data <- data[-train_idx, ]

# Basic preprocessing for glmnet
x_train <- model.matrix(obese ~ . - 1, data = train_data)
y_train <- as.numeric(train_data$obese) - 1
x_test <- model.matrix(obese ~ . - 1, data = test_data)
y_test <- test_data$obese

# ----------------------
# BAGGING: Random Forest
# ----------------------
rf_fit <- ranger(obese ~ ., data = train_data, probability = TRUE)
rf_probs <- predict(rf_fit, data = test_data)$predictions[, "Yes"]

# ----------------------
# BOOSTING: XGBoost
# ----------------------
dtrain <- xgb.DMatrix(data = x_train, label = y_train)
dtest <- xgb.DMatrix(data = x_test)
xgb_model <- xgboost(data = dtrain, nrounds = 100, objective = "binary:logistic", verbose = 0)
xgb_probs <- predict(xgb_model, newdata = dtest)

# ----------------------
# SCALING FOR KNN
# ----------------------
scale_train <- scale(train_data[, -which(names(train_data) == "obese")])
scale_test <- scale(test_data[, -which(names(test_data) == "obese")],
                    center = attr(scale_train, "scaled:center"),
                    scale = attr(scale_train, "scaled:scale"))

# ----------------------
# STACKING BASE MODELS
# ----------------------
# LASSO
lasso_fit <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 1)
lasso_probs <- predict(lasso_fit, newx = x_test, s = "lambda.min", type = "response")

# SVM
svm_fit <- svm(obese ~ ., data = train_data, probability = TRUE)
svm_pred <- predict(svm_fit, newdata = test_data, probability = TRUE)
svm_probs <- attr(svm_pred, "probabilities")[, "Yes"]

# KNN
knn_pred <- knn(train = scale_train, test = scale_test,
                cl = train_data$obese, k = 5, prob = TRUE)
knn_probs <- ifelse(knn_pred == "Yes",
                    attr(knn_pred, "prob"),
                    1 - attr(knn_pred, "prob"))

# Prepare training data for stacking meta-model
stack_train <- data.frame(
  lasso = predict(lasso_fit, newx = x_train, s = "lambda.min", type = "response")[,1],
  rf = predict(rf_fit, data = train_data)$predictions[, "Yes"],
  svm = attr(predict(svm_fit, newdata = train_data, probability = TRUE), "probabilities")[, "Yes"],
  knn = ifelse(knn(train = scale_train, test = scale_train,
                   cl = train_data$obese, k = 5, prob = TRUE) == "Yes",
               attr(knn(train = scale_train, test = scale_train,
                         cl = train_data$obese, k = 5, prob = TRUE), "prob"),
               1 - attr(knn(train = scale_train, test = scale_train,
                         cl = train_data$obese, k = 5, prob = TRUE), "prob")),
  obese = train_data$obese
)

# Meta-model: logistic regression
meta_model <- glm(obese ~ ., data = stack_train, family = binomial)

# Test set stacking inputs
stack_test <- data.frame(
  lasso = lasso_probs,
  rf = rf_probs,
  svm = svm_probs,
  knn = knn_probs
)

# Predict using stacked model
stack_probs <- predict(meta_model, newdata = stack_test, type = "response")
stack_preds <- ifelse(stack_probs > 0.5, "Yes", "No") |> factor(levels = c("No", "Yes"))

# Accuracy evaluation
lasso_acc <- mean(ifelse(lasso_probs > 0.5, "Yes", "No") == y_test)
rf_acc <- mean(ifelse(rf_probs > 0.5, "Yes", "No") == y_test)
xgb_acc <- mean(ifelse(xgb_probs > 0.5, "Yes", "No") == y_test)
svm_acc <- mean(ifelse(svm_probs > 0.5, "Yes", "No") == y_test)
knn_acc <- mean(ifelse(knn_probs > 0.5, "Yes", "No") == y_test)
stack_acc <- mean(stack_preds == y_test)

# Print accuracy results
results <- data.frame(
  Model = c("LASSO", "Random Forest (Bagging)", "XGBoost (Boosting)", "SVM", "KNN", "Stacked Ensemble"),
  Accuracy = c(lasso_acc, rf_acc, xgb_acc, svm_acc, knn_acc, stack_acc)
)

print(results)

```

