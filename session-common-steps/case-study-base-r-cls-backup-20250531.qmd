---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Demo: a predictive modelling case study 

Let's use base R and selected packages to build a predictive model for BMI using our `diabetes` data set. 

## Simulate data

```{r}
library(tidyverse)

# Load the data
input_diabetes <- read_csv("data/data-diabetes.csv")

# Calculate median BMI to replace (temporary missing values)
BMI_median <- median(input_diabetes$weight / (input_diabetes$height^2) * 703, na.rm = TRUE)

# Create BMI variable
data_diabetes <- input_diabetes %>%
  mutate(BMI = round(weight / height^2 * 703, 2)) %>%
  relocate(BMI, .after = id) %>%
  mutate(BMI = replace_na(BMI, BMI_median)) %>%
  mutate(obese = cut(BMI, breaks = c(0, 29.9, 100), labels = c("No", "Yes"))) %>%
  mutate(obese = factor(obese, levels = c("No", "Yes")))

#write_csv(data_diabetes, "data_diabetes_obese.csv")

# Add simulated gene expression data

# Load your dataset
#data <- read.csv("data_diabetes_obese.csv")
data <- data_diabetes

genes_hsa <- read.csv("counts_anno.csv") %>%
  filter(hgnc_symbol != "") %>%
  pull(hgnc_symbol)

# Ensure 'obese' is a factor
data$obese <- factor(data$obese, levels = c("No", "Yes"))

# Set parameters
set.seed(123)
n_samples <- nrow(data)
n_genes <- 1000
n_diff <- 6

# Select some known obesity genes to include in the differentially expressed set
obesity_genes <- c("FTO", "MC4R", "LEP", "LEPR", "POMC", "PCSK1", "BDNF", "TMEM18", "NEGR1", "SH2B1")
obesity_genes <- c("FTO", "MC4R", "LEP", "LEPR", "POMC", "PCSK1")

# Select additional DE genes
n_additional <- n_diff - length(obesity_genes)
other_diff_genes <- sample(setdiff(genes_hsa, obesity_genes), n_additional)
diff_gene_names <- c(obesity_genes, other_diff_genes)

# Remaining non-differentially expressed genes
non_diff_gene_names <- sample(setdiff(genes_hsa, diff_gene_names), n_genes - n_diff)

# All genes
all_gene_names <- sample(c(diff_gene_names, non_diff_gene_names), n_genes)

# Simulate expression matrix (normal distribution)
sim_matrix <- matrix(rnorm(n_samples * n_genes, mean = 0, sd = 1), 
                     nrow = n_samples, ncol = n_genes)
colnames(sim_matrix) <- all_gene_names

# Add differential expression for selected genes
for (gene in diff_gene_names) {
  idx <- which(colnames(sim_matrix) == gene)
  sim_matrix[data$obese == "Yes", idx] <- rnorm(sum(data$obese == "Yes"), mean = 1, sd = 1)
  sim_matrix[data$obese == "No", idx]  <- rnorm(sum(data$obese == "No"), mean = 0, sd = 1)
}

for (gene in obesity_genes) {
  idx <- which(colnames(sim_matrix) == gene)
  sim_matrix[data$obese == "Yes", idx] <- rnorm(sum(data$obese == "Yes"), mean = 1, sd = 1)
  sim_matrix[data$obese == "No", idx]  <- rnorm(sum(data$obese == "No"), mean = 0, sd = 1)
}


# Convert to data frame and combine with original data
gene_df <- as.data.frame(sim_matrix)
gene_df$id = data$id  

combined_data <- cbind(data, gene_df)

# Preview the result
head(combined_data[, c("obese", diff_gene_names)])

# Reintroduce missing data
idx <- which(data$BMI == BMI_median)
data$BMI[idx] <- NA
data$obese[idx] <- NA
data <- data %>%
  relocate(obese, .after = BMI)

# Save to CSV if needed
write_csv(data, "data-obesity.csv")
write_csv(gene_df, "data-obesity-genes.csv")

```

## Load Data and Perform EDA
```{r}
#| label: load-data
#| eval: true
#| warning: false
#| message: false
#| code-fold: false
#| collapse: true
#| fig-show: hold
#| fig-cap-location: margin
#| fig-cap: 
#| - "Number of missing data per variable, shows that bp.2d and bp.2s have more than 50% missing entries"
#| - "Heatmap visualizing Pearson correlation coefficient between numerical variables"

#rm(list=ls())

library(tidyverse)
library(ggcorrplot)
library(glmnet)
library(fastDummies)

# Load the clinical data
data_obesity <- read_csv("data-obesity.csv") %>%
  select(-BMI, -hip)
data_expr <- read_csv("data-obesity-genes.csv")

# preview data
glimpse(data_obesity)

# run basic EDA
# note: we have seen descriptive statistics and plots during EDA session 
# note: so here we only look at missing data and correlation

# calculate number of missing data per variable
data_na <- data_obesity %>% 
  summarise(across(everything(), ~ sum(is.na(.)))) 

# make a table with counts sorted from highest to lowest
data_na_long <- data_na %>%
  pivot_longer(-id, names_to = "variable", values_to = "count") %>%
  arrange(desc(count)) 

# make a column plot to visualize the counts
data_na_long %>%
  ggplot(aes(x = variable, y = count)) + 
  geom_col(fill = "blue4") + 
  xlab("") + 
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))

# Based on the number of missing data, let's delete bp.2s, bp.2d
# and use complete-cases analysis 
data_obesity <- data_obesity %>%
  dplyr::select(-bp.2s, -bp.2d) %>%
  na.omit()

# Correlation heatmap
data_cor <- data_obesity %>%
  select(where(is.numeric), -id) %>%
  cor()

ggcorrplot(data_cor, hc.order = TRUE, lab = FALSE)
```

```{r}

# Load gene expression data
# data_expr <- read_csv("data-obesity-genes.csv")

# Join with clinical data
data <- data_obesity %>%
  left_join(data_expr, by = "id")

```

## Split Data
```{r}
#| label: split-data
#| eval: true
#| warning: false
#| message: false
#| code-fold: false

set.seed(123)
n <- nrow(data)
test_index <- sample(seq_len(n), size = 0.2 * n)
data_test <- data[test_index, ]
data_train <- data[-test_index, ]
```

## Feature Engineering and Scaling
```{r}
#| label: feature-eng
#| eval: true
#| warning: false
#| message: false
#| code-fold: false

# Conversion factors
inch2m <- 2.54 / 100
pound2kg <- 0.45

# ---- Process Training Data ----
data_train_processed <- data_train %>%
  mutate(
    height = round(height * inch2m, 2),
    weight = round(weight * pound2kg, 2),
    glu = log(stab.glu)
  ) %>%
  select(-stab.glu, -id)

# Remove zero-variance features
nzv <- sapply(data_train_processed, function(x) length(unique(x)) > 1)
data_train_processed <- data_train_processed[, nzv]

# Remove highly correlated predictors (|r| > 0.8)
cor_matrix <- cor(select(data_train_processed, where(is.numeric)))
high_corr <- names(which(apply(cor_matrix, 2, function(x) any(abs(x) > 0.8 & abs(x) < 1))))

#data_train_processed <- data_train_processed %>% select(-all_of(high_corr))
data_train_processed <- data_train_processed %>% select(-c("weight", "waist"))

# Dummy encode categorical variables
data_train_processed <- dummy_cols(data_train_processed,
                                   select_columns = c("location", "gender", "frame"),
                                   remove_selected_columns = TRUE)

# Separate outcome and predictors
y_train <- data_train_processed$obese
x_train <- data_train_processed %>% select(-obese)

# Scale predictors
x_train_scaled <- scale(x_train)
train_means <- attr(x_train_scaled, "scaled:center")
train_sds <- attr(x_train_scaled, "scaled:scale")
```

## Prepare Test Data with Same Processing
```{r}
#| label: process-test-data
#| eval: true
#| warning: false
#| message: false
#| code-fold: false

# ---- Process Test Data ----
data_test_processed <- data_test %>%
  mutate(
    height = round(height * inch2m, 2),
    weight = round(weight * pound2kg, 2),
    glu = log(stab.glu)
  ) %>%
  select(-stab.glu, -id)


# Dummy encode categorical variables
data_test_processed <- dummy_cols(data_test_processed,
                                  select_columns = c("location", "gender", "frame"),
                                  remove_selected_columns = TRUE)

# Remove same columns as in training
data_test_processed <- data_test_processed %>%
  select(colnames(data_train_processed))

# Ensure same column order as training
x_test <- data_test_processed %>% select(-obese)
x_test <- x_test[, colnames(x_train)]

# Apply training set scaling
x_test_scaled <- scale(x_test, center = train_means, scale = train_sds)
y_test <- data_test_processed$obese

```

## Lasso Regression with Cross-Validation
```{r}
#| label: fit-lassso
#| eval: true
#| warning: false
#| message: false
#| code-fold: false

# Fit Lasso regression with 10-fold CV
set.seed(123)
#y_train <- ifelse(y_train == "Yes", 1, 0)  
#y_train <- as.factor(y_train)
cv_model <- cv.glmnet(x_train_scaled, y_train, alpha = 1, standardize = FALSE, family = "binomial")

# Plot cross-validation error
plot(cv_model)

# Best lambda value
best_lambda <- cv_model$lambda.1se
cat("Best lambda:", best_lambda)
```

## Evaluate Model on Test Data
```{r}
#| label: evaluate-model
#| eval: true
#| warning: false
#| message: false
#| code-fold: false

# Predict
pred_test <- predict(cv_model, s = best_lambda, newx = x_test_scaled, type = "response")
print(head(pred_test))
y_pred <- ifelse(pred_test > 0.5, "Yes", "No")

# Error
table(y_test, y_pred)


```


```{r}
library(pROC)       # For ROC and AUC

# Predictions (from earlier)
pred_probs <- predict(cv_model, newx = x_test_scaled, s = best_lambda, type = "response")
pred_labels <- ifelse(pred_probs >= 0.5, 1, 0)


# confusion matrix
conf_matrix <- table(Predicted = y_pred, Actual = y_test)
print(conf_matrix)

# accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Accuracy:", round(accuracy, 3), "\n")

# ---- Compute Precision, Recall, F1 ----
TP <- conf_matrix["Yes", "Yes"]
FP <- conf_matrix["Yes", "No"]
FN <- conf_matrix["No", "Yes"]

precision <- TP / (TP + FP)
recall    <- TP / (TP + FN)
f1_score  <- 2 * (precision * recall) / (precision + recall)

cat("Precision:", round(precision, 3), "\n")
cat("Recall:", round(recall, 3), "\n")
cat("F1 Score:", round(f1_score, 3), "\n")

# ---- ROC Curve and AUC (Still use pROC) ----
y_test_numeric <- ifelse(y_test == "Yes", 1, 0)  # Convert to numeric for ROC
y_pred_numeric <- as.numeric(ifelse(y_pred == "Yes", 1, 0))  # Convert to numeric for ROC

roc_obj <- roc(y_test_numeric, as.numeric(pred_probs))
plot(roc_obj, main = "ROC Curve", col = "blue", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "gray")

auc_val <- auc(roc_obj)
cat("AUC:", round(auc_val, 3), "\n")



```

## Variable Importance Plot
```{r}
#| label: vip
#| eval: true
#| warning: false
#| message: false
#| code-fold: false

# Extract coefficients
coef_matrix <- coef(cv_model, s = best_lambda)
coef_df <- as.data.frame(as.matrix(coef_matrix))
coef_df$feature <- rownames(coef_df)
colnames(coef_df)[1] <- "coefficient"

# Filter out intercept and zero coefficients
coef_df <- coef_df %>%
  filter(feature != "(Intercept)", coefficient != 0) %>%
  mutate(abs_coef = abs(coefficient)) %>%
  arrange(desc(abs_coef))

# Plot
ggplot(coef_df, aes(x = reorder(feature, abs_coef), y = abs_coef)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Variable Importance (Lasso Coefficients)",
       x = "Feature", y = "Absolute Coefficient") +
  theme_minimal()
```

```{r}

print(intersect(coef_df$feature, diff_gene_names))
print(length(intersect(coef_df$feature, diff_gene_names)))

print(intersect(coef_df$feature, obesity_genes))
print(length(intersect(coef_df$feature, obesity_genes)))

```

## Random Forest

```{r}
library(ranger)

# Combine predictors and target into a single data frame for ranger
train_df <- data.frame(y = factor(y_train), x_train_scaled)

# Fit the random forest model
set.seed(123)
rf_model <- ranger(
  formula = y ~ ., 
  data = train_df,
  probability = TRUE,         # Enable probability predictions for ROC
  num.trees = 500,
  importance = "impurity"
)

```

```{r}
# Create test data frame with same structure
test_df <- data.frame(x_test_scaled)

# Predict probabilities for the positive class
rf_probs <- predict(rf_model, data = test_df)$predictions[, "Yes"]

# Binary predictions using threshold
rf_preds <- ifelse(rf_probs >= 0.5, "Yes", "No")

# Confusion matrix (base R)
conf_matrix <- table(Predicted = rf_preds, Actual = y_test)
print(conf_matrix)

# Accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Accuracy:", round(accuracy, 3), "\n")

# ROC and AUC
roc_obj <- roc(y_test, rf_probs)
plot(roc_obj, col = "blue", lwd = 2, main = "Random Forest ROC")
abline(a = 0, b = 1, lty = 2, col = "gray")
cat("AUC:", auc(roc_obj), "\n")

```

```{r}
library(ggplot2)

# feature importance
importance <- rf_model$variable.importance

# Convert to data frame
importance_df <- data.frame(
  feature = names(importance),
  importance = importance
)

# Plot top features
importance_df <- importance_df %>%
  arrange(desc(importance))

ggplot(importance_df[1:20, ], aes(x = reorder(feature, importance), y = importance)) +
  geom_col(fill = "darkgreen") +
  coord_flip() +
  labs(
    title = "Variable Importance (Random Forest)",
    x = "Feature", y = "Importance (Gini Impurity)"
  ) +
  theme_minimal()

```

## Tune RF
```{r}
library(ranger)
library(pROC)

# Create a function to evaluate AUC for a set of parameters
tune_rf <- function(mtry_val, min_node) {
  rf_model <- ranger(
    y ~ ., data = train_df,
    probability = TRUE,
    num.trees = 500,
    mtry = mtry_val,
    min.node.size = min_node,
    seed = 123
  )
  
  rf_probs <- predict(rf_model, data = test_df)$predictions[, "Yes"]
  roc_obj <- roc(y_test, rf_probs, quiet = TRUE)
  auc_val <- auc(roc_obj)
  return(auc_val)
}

# Grid search over mtry and min.node.size
results <- expand.grid(
  mtry = c(5, 10, 20, 50),
  min.node.size = c(1, 5, 10)
)

results$AUC <- mapply(tune_rf, results$mtry, results$min.node.size)

# View sorted results
results[order(-results$AUC), ]

```


