---
title: "BMI Prediction without Tidymodels"
output: html_document
editor_options:
  chunk_output_type: console
---

# Demo: a predictive modelling case study (base R)

Here, we will build a predictive model for BMI, given our diabetes dataset. We will use base R and selected modeling packages directly for data preprocessing, model training, cross-validation, and evaluation.

## Load Data and perform EDA
```{r}
library(tidyverse)
library(ggcorrplot)
library(glmnet)

# Load the data
input_diabetes <- read_csv("data/data-diabetes.csv")

# create BMI variable
conv_factor <- 703 # conversion factor to calculate BMI from inches and pounds BMI = weight (lb) / [height (in)]2 x 703
data_diabetes <- input_diabetes %>%
  mutate(BMI = weight / height^2 * 703, BMI = round(BMI, 2)) %>%
  relocate(BMI, .after = id)

# preview data
glimpse(data_diabetes)

# run basic EDA
# note: we have seen descriptive statistics and plots during EDA session 
# note: so here we only look at missing data and correlation

# calculate number of missing data per variable
data_na <- data_diabetes %>% 
  summarise(across(everything(), ~ sum(is.na(.)))) 

# make a table with counts sorted from highest to lowest
data_na_long <- data_na %>%
  pivot_longer(-id, names_to = "variable", values_to = "count") %>%
  arrange(desc(count)) 

# make a column plot to visualize the counts
data_na_long %>%
  ggplot(aes(x = variable, y = count)) + 
  geom_col(fill = "blue4") + 
  xlab("") + 
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))

# We see that bp.1s and bp.2d measurements are missing in more than 50% of the samples.
# We will omit these variables and retain only the samples with complete measurements (complete-case analysis)

data_diabetes <- data_diabetes %>%
  select(-bp.2s, -bp.2d) %>%
  na.omit()

# Correlation heatmap
data_cor <- data_diabetes %>%
  select(where(is.numeric), -id) %>%
  cor()

ggcorrplot(data_cor, hc.order = TRUE, lab = FALSE)
```

## Feature Engineering
```{r}
# Convert height to meters, weight to kg
inch2m <- 2.54/100
pound2kg <- 0.45

data_diabetes <- data_diabetes %>%
  mutate(height = round(height * inch2m, 2),
         weight = round(weight * pound2kg, 2),
         glu = log(stab.glu)) %>%
  select(-stab.glu)

# Drop highly correlated features manually if needed (based on heatmap)
```

## Split Data
```{r}
set.seed(123)
n <- nrow(data_diabetes)
test_index <- sample(seq_len(n), size = 0.2 * n)
data_test <- data_diabetes[test_index, ]
data_train <- data_diabetes[-test_index, ]

# check the split
dim(data_diabetes)
dim(data_train)
dim(data_test)

# check BMI distributions in data splits
par(mfrow=c(3,1))
hist(data_diabetes$BMI, xlab = "", main = "BMI: all", 50)
hist(data_train$BMI, xlab = "", main = "BMI: non-test", 50)
hist(data_test$BMI, xlab = "", main = "BMI: test", 50)

```

## Prepare Design Matrix
```{r}
# Remove ID and outcome column for matrix
x_train <- model.matrix(BMI ~ . - id, data = data_train)[, -1]
y_train <- data_train$BMI

x_test <- model.matrix(BMI ~ . - id, data = data_test)[, -1]
y_test <- data_test$BMI
```

## Lasso Regression with Cross-Validation
```{r}
cv_model <- cv.glmnet(x_train, y_train, alpha = 1)

# Plot cross-validation error vs lambda
plot(cv_model)

# Best lambda
best_lambda <- cv_model$lambda.min
```

## Evaluate Model
```{r}
# Predict on test data
pred_test <- predict(cv_model, s = best_lambda, newx = x_test)

# RMSE
rmse <- sqrt(mean((pred_test - y_test)^2))
cat("RMSE on test data:", rmse, "\n")

# Correlation
cor(pred_test, y_test)

# Plot predictions
plot(y_test, pred_test, xlab = "Actual BMI", ylab = "Predicted BMI", pch = 19)
abline(0, 1, col = "red")
```


## Visualize Variable Importance
```{r}
# Extract coefficients from the model at best lambda
coef_matrix <- coef(cv_model, s = best_lambda)

# Convert to tidy data frame
coef_df <- as.data.frame(as.matrix(coef_matrix))
coef_df$feature <- rownames(coef_df)
colnames(coef_df)[1] <- "coefficient"

# Filter out the intercept and zero coefficients
coef_df <- coef_df %>%
  filter(feature != "(Intercept)", coefficient != 0) %>%
  mutate(abs_coef = abs(coefficient)) %>%
  arrange(desc(abs_coef))

# Plot top variables by absolute coefficient value
library(ggplot2)
ggplot(coef_df, aes(x = reorder(feature, abs_coef), y = abs_coef)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Variable Importance (Lasso Coefficients)",
       x = "Feature",
       y = "Absolute Coefficient") +
  theme_minimal()

```
