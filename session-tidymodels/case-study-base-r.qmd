---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Demo: a predictive modelling case study (base R approach)

Let's use base R and selected packages to build a predictive model for BMI using our `diabetes` data set. 

## Load Data and Perform EDA
```{r}
#| label: load-data
#| eval: true
#| warning: false
#| message: false
#| code-fold: false
#| collapse: true
#| fig-show: hold
#| fig-cap-location: margin
#| fig-cap: 
#| - "Number of missing data per variable, shows that bp.2d and bp.2s have more than 50% missing entries"
#| - "Heatmap visualizing Pearson correlation coefficient between numerical variables"

library(tidyverse)
library(ggcorrplot)
library(glmnet)
library(fastDummies)

# Load the data
input_diabetes <- read_csv("data/data-diabetes.csv")

# Create BMI variable
data_diabetes <- input_diabetes %>%
  mutate(BMI = round(weight / height^2 * 703, 2)) %>%
  relocate(BMI, .after = id)

# preview data
glimpse(data_diabetes)

# run basic EDA
# note: we have seen descriptive statistics and plots during EDA session 
# note: so here we only look at missing data and correlation

# calculate number of missing data per variable
data_na <- data_diabetes %>% 
  summarise(across(everything(), ~ sum(is.na(.)))) 

# make a table with counts sorted from highest to lowest
data_na_long <- data_na %>%
  pivot_longer(-id, names_to = "variable", values_to = "count") %>%
  arrange(desc(count)) 

# make a column plot to visualize the counts
data_na_long %>%
  ggplot(aes(x = variable, y = count)) + 
  geom_col(fill = "blue4") + 
  xlab("") + 
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))

# Based on the number of missing data, let's delete bp.2s, bp.2d
# and use complete-cases analysis 
data_diabetes <- data_diabetes %>%
  dplyr::select(-bp.2s, -bp.2d) %>%
  na.omit()

# Correlation heatmap
data_cor <- data_diabetes %>%
  select(where(is.numeric), -id) %>%
  cor()

ggcorrplot(data_cor, hc.order = TRUE, lab = FALSE)
```

## Split Data
```{r}
#| label: split-data
#| eval: true
#| warning: false
#| message: false
#| code-fold: false

set.seed(123)
n <- nrow(data_diabetes)
test_index <- sample(seq_len(n), size = 0.2 * n)
data_test <- data_diabetes[test_index, ]
data_train <- data_diabetes[-test_index, ]
```

## Feature Engineering and Scaling
```{r}
#| label: feature-eng
#| eval: true
#| warning: false
#| message: false
#| code-fold: false

# Conversion factors
inch2m <- 2.54 / 100
pound2kg <- 0.45

# ---- Process Training Data ----
data_train_processed <- data_train %>%
  mutate(
    height = round(height * inch2m, 2),
    weight = round(weight * pound2kg, 2),
    glu = log(stab.glu)
  ) %>%
  select(-stab.glu, -id)

# Remove zero-variance features
nzv <- sapply(data_train_processed, function(x) length(unique(x)) > 1)
data_train_processed <- data_train_processed[, nzv]

# Remove highly correlated predictors (|r| > 0.8)
cor_matrix <- cor(select(data_train_processed, where(is.numeric)))
high_corr <- names(which(apply(cor_matrix, 2, function(x) any(abs(x) > 0.8 & abs(x) < 1))))

#data_train_processed <- data_train_processed %>% select(-all_of(high_corr))
data_train_processed <- data_train_processed %>% select(-c("weight", "waist"))

# Dummy encode categorical variables
data_train_processed <- dummy_cols(data_train_processed,
                                   select_columns = c("location", "gender", "frame"),
                                   remove_selected_columns = TRUE)

# Separate outcome and predictors
y_train <- data_train_processed$BMI
x_train <- data_train_processed %>% select(-BMI)

# Scale predictors
x_train_scaled <- scale(x_train)
train_means <- attr(x_train_scaled, "scaled:center")
train_sds <- attr(x_train_scaled, "scaled:scale")
```

## Prepare Test Data with Same Processing
```{r}
#| label: process-test-data
#| eval: true
#| warning: false
#| message: false
#| code-fold: false

# ---- Process Test Data ----
data_test_processed <- data_test %>%
  mutate(
    height = round(height * inch2m, 2),
    weight = round(weight * pound2kg, 2),
    glu = log(stab.glu)
  ) %>%
  select(-stab.glu, -id)


# Dummy encode categorical variables
data_test_processed <- dummy_cols(data_test_processed,
                                  select_columns = c("location", "gender", "frame"),
                                  remove_selected_columns = TRUE)

# Remove same columns as in training
data_test_processed <- data_test_processed %>%
  select(colnames(data_train_processed))

# Ensure same column order as training
x_test <- data_test_processed %>% select(-BMI)
x_test <- x_test[, colnames(x_train)]

# Apply training set scaling
x_test_scaled <- scale(x_test, center = train_means, scale = train_sds)
y_test <- data_test_processed$BMI
```

## Lasso Regression with Cross-Validation
```{r}
#| label: fit-lassso
#| eval: true
#| warning: false
#| message: false
#| code-fold: false

# Fit Lasso regression with 10-fold CV
set.seed(123)
cv_model <- cv.glmnet(x_train_scaled, y_train, alpha = 1, standardize = FALSE)

# Plot cross-validation error
plot(cv_model)

# Best lambda value
best_lambda <- cv_model$lambda.min
cat("Best lambda:", best_lambda)
```

## Evaluate Model on Test Data
```{r}
#| label: evaluate-model
#| eval: true
#| warning: false
#| message: false
#| code-fold: false

# Predict
pred_test <- predict(cv_model, s = best_lambda, newx = x_test_scaled)

# RMSE
rmse <- sqrt(mean((pred_test - y_test)^2))
cat("RMSE on test data:", rmse)

# Correlation
cor(pred_test, y_test)

# Scatter plot: Predicted vs Actual
plot(y_test, pred_test,
     xlab = "Actual BMI", ylab = "Predicted BMI", pch = 19,
     main = "Predicted vs Actual BMI")
abline(0, 1, col = "red", lwd = 2)
```

## Variable Importance Plot
```{r}
#| label: vip
#| eval: true
#| warning: false
#| message: false
#| code-fold: false


# Extract coefficients
coef_matrix <- coef(cv_model, s = best_lambda)
coef_df <- as.data.frame(as.matrix(coef_matrix))
coef_df$feature <- rownames(coef_df)
colnames(coef_df)[1] <- "coefficient"

# Filter out intercept and zero coefficients
coef_df <- coef_df %>%
  filter(feature != "(Intercept)", coefficient != 0) %>%
  mutate(abs_coef = abs(coefficient)) %>%
  arrange(desc(abs_coef))

# Plot
ggplot(coef_df, aes(x = reorder(feature, abs_coef), y = abs_coef)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Variable Importance (Lasso Coefficients)",
       x = "Feature", y = "Absolute Coefficient") +
  theme_minimal()
```
