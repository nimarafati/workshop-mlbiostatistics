[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Putting things together",
    "section": "",
    "text": "Preface\nAims\n\nto introduce tidymodels framework for predictive modelling studies\nand show how to put all the common steps for building predictive model\n\nLearning outcomes\n\nto be able to use tidymodels framework for a complete workflow of supervised learning\n\nDo you see a mistake or a typo? We would be grateful if you let us know via edu.ml-biostats@nbis.se\nThis repository contains teaching and learning materials prepared for and used during “Introduction to biostatistics and Machine Learning” course, organized by NBIS, National Bioinformatics Infrastructure Sweden. The course is open for PhD students, postdoctoral researcher and other employees within Swedish universities. The materials are geared towards life scientists wanting to be able to understand and use the basic statistical and machine learning methods. More about the course https://nbisweden.github.io/workshop-mlbiostatistics/",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Tidymodels",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#tidymodels",
    "href": "intro.html#tidymodels",
    "title": "1  Introduction",
    "section": "",
    "text": "One of the earlier initiatives to create a framwork for ML tasks in R was the caret package, led by Max Kuhn, which unified many modeling tools and provided support for preprocessing, resampling, and parameter tuning. Caretwas an early and widely-used framework that provided tools for preprocessing, resampling, and cross-validation.\nBuilding on this foundation, Kuhn partnered with Hadley Wickham, the creator of the tidyverse, to introduce the tidymodels ecosystem in 2020: a modern, modular collection of R packages that applies tidyverse principles to make machine learning workflows more intuitive, readable, and consistent.\n\n\nSome of the core packages under `tidymodels` framework https://www.tidymodels.org\n\n\n\n\n\n\ncore package\nfunction\n\n\n\n\n\nprovides infrastructure for efficient data splitting and resampling\n\n\n\nparsnip is a tidy, unified interface to models that can be used to try a range of models without getting bogged down in the syntactical minutiae of the underlying packages\n\n\n\nrecipes is a tidy interface to data pre-processing tools for feature engineering\n\n\n\nworkflows bundle your pre-processing, modeling, and post-processing together\n\n\n\ntune helps you optimize the hyperparameters of your model and pre-processing steps\n\n\n\nyardstick measures the effectiveness of models using performance metrics",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#references",
    "href": "intro.html#references",
    "title": "1  Introduction",
    "section": "References",
    "text": "References",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "case-study-base-r.html",
    "href": "case-study-base-r.html",
    "title": "2  Demo: a predictive modelling case study (base R approach)",
    "section": "",
    "text": "2.1 Load Data and Perform EDA\nLet’s use base R and selected packages to build a predictive model for BMI using our diabetes data set.\nlibrary(tidyverse)\nlibrary(ggcorrplot)\nlibrary(glmnet)\nlibrary(fastDummies)\n\n# Load the data\ninput_diabetes &lt;- read_csv(\"data/data-diabetes.csv\")\n\n# Create BMI variable\ndata_diabetes &lt;- input_diabetes %&gt;%\n  mutate(BMI = round(weight / height^2 * 703, 2)) %&gt;%\n  relocate(BMI, .after = id)\n\n# preview data\nglimpse(data_diabetes)\n## Rows: 403\n## Columns: 20\n## $ id       &lt;dbl&gt; 1000, 1001, 1002, 1003, 1005, 1008, 1011, 1015, 1016, 1022, 1…\n## $ BMI      &lt;dbl&gt; 22.13, 37.42, 48.37, 18.64, 27.82, 26.50, 28.20, 34.33, 24.51…\n## $ chol     &lt;dbl&gt; 203, 165, 228, 78, 249, 248, 195, 227, 177, 263, 242, 215, 23…\n## $ stab.glu &lt;dbl&gt; 82, 97, 92, 93, 90, 94, 92, 75, 87, 89, 82, 128, 75, 79, 76, …\n## $ hdl      &lt;dbl&gt; 56, 24, 37, 12, 28, 69, 41, 44, 49, 40, 54, 34, 36, 46, 30, 4…\n## $ ratio    &lt;dbl&gt; 3.6, 6.9, 6.2, 6.5, 8.9, 3.6, 4.8, 5.2, 3.6, 6.6, 4.5, 6.3, 6…\n## $ glyhb    &lt;dbl&gt; 4.31, 4.44, 4.64, 4.63, 7.72, 4.81, 4.84, 3.94, 4.84, 5.78, 4…\n## $ location &lt;chr&gt; \"Buckingham\", \"Buckingham\", \"Buckingham\", \"Buckingham\", \"Buck…\n## $ age      &lt;dbl&gt; 46, 29, 58, 67, 64, 34, 30, 37, 45, 55, 60, 38, 27, 40, 36, 3…\n## $ gender   &lt;chr&gt; \"female\", \"female\", \"female\", \"male\", \"male\", \"male\", \"male\",…\n## $ height   &lt;dbl&gt; 62, 64, 61, 67, 68, 71, 69, 59, 69, 63, 65, 58, 60, 59, 69, 6…\n## $ weight   &lt;dbl&gt; 121, 218, 256, 119, 183, 190, 191, 170, 166, 202, 156, 195, 1…\n## $ frame    &lt;chr&gt; \"medium\", \"large\", \"large\", \"large\", \"medium\", \"large\", \"medi…\n## $ bp.1s    &lt;dbl&gt; 118, 112, 190, 110, 138, 132, 161, NA, 160, 108, 130, 102, 13…\n## $ bp.1d    &lt;dbl&gt; 59, 68, 92, 50, 80, 86, 112, NA, 80, 72, 90, 68, 80, NA, 66, …\n## $ bp.2s    &lt;dbl&gt; NA, NA, 185, NA, NA, NA, 161, NA, 128, NA, 130, NA, NA, NA, N…\n## $ bp.2d    &lt;dbl&gt; NA, NA, 92, NA, NA, NA, 112, NA, 86, NA, 90, NA, NA, NA, NA, …\n## $ waist    &lt;dbl&gt; 29, 46, 49, 33, 44, 36, 46, 34, 34, 45, 39, 42, 35, 37, 36, 3…\n## $ hip      &lt;dbl&gt; 38, 48, 57, 38, 41, 42, 49, 39, 40, 50, 45, 50, 41, 43, 40, 4…\n## $ time.ppn &lt;dbl&gt; 720, 360, 180, 480, 300, 195, 720, 1020, 300, 240, 300, 90, 7…\n\n# run basic EDA\n# note: we have seen descriptive statistics and plots during EDA session \n# note: so here we only look at missing data and correlation\n\n# calculate number of missing data per variable\ndata_na &lt;- data_diabetes %&gt;% \n  summarise(across(everything(), ~ sum(is.na(.)))) \n\n# make a table with counts sorted from highest to lowest\ndata_na_long &lt;- data_na %&gt;%\n  pivot_longer(-id, names_to = \"variable\", values_to = \"count\") %&gt;%\n  arrange(desc(count)) \n\n# make a column plot to visualize the counts\ndata_na_long %&gt;%\n  ggplot(aes(x = variable, y = count)) + \n  geom_col(fill = \"blue4\") + \n  xlab(\"\") + \n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\n# Based on the number of missing data, let's delete bp.2s, bp.2d\n# and use complete-cases analysis \ndata_diabetes &lt;- data_diabetes %&gt;%\n  dplyr::select(-bp.2s, -bp.2d) %&gt;%\n  na.omit()\n\n# Correlation heatmap\ndata_cor &lt;- data_diabetes %&gt;%\n  select(where(is.numeric), -id) %&gt;%\n  cor()\n\nggcorrplot(data_cor, hc.order = TRUE, lab = FALSE)\n\n\n\n\nNumber of missing data per variable, shows that bp.2d and bp.2s have more than 50% missing entries\n\n\n\n\n\n\n\nHeatmap visualizing Pearson correlation coefficient between numerical variables",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Demo: a predictive modelling case study (base R approach)</span>"
    ]
  },
  {
    "objectID": "case-study-base-r.html#split-data",
    "href": "case-study-base-r.html#split-data",
    "title": "2  Demo: a predictive modelling case study (base R approach)",
    "section": "2.2 Split Data",
    "text": "2.2 Split Data\n\nset.seed(123)\nn &lt;- nrow(data_diabetes)\ntest_index &lt;- sample(seq_len(n), size = 0.2 * n)\ndata_test &lt;- data_diabetes[test_index, ]\ndata_train &lt;- data_diabetes[-test_index, ]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Demo: a predictive modelling case study (base R approach)</span>"
    ]
  },
  {
    "objectID": "case-study-base-r.html#feature-engineering-and-scaling",
    "href": "case-study-base-r.html#feature-engineering-and-scaling",
    "title": "2  Demo: a predictive modelling case study (base R approach)",
    "section": "2.3 Feature Engineering and Scaling",
    "text": "2.3 Feature Engineering and Scaling\n\n# Conversion factors\ninch2m &lt;- 2.54 / 100\npound2kg &lt;- 0.45\n\n# ---- Process Training Data ----\ndata_train_processed &lt;- data_train %&gt;%\n  mutate(\n    height = round(height * inch2m, 2),\n    weight = round(weight * pound2kg, 2),\n    glu = log(stab.glu)\n  ) %&gt;%\n  select(-stab.glu, -id)\n\n# Remove zero-variance features\nnzv &lt;- sapply(data_train_processed, function(x) length(unique(x)) &gt; 1)\ndata_train_processed &lt;- data_train_processed[, nzv]\n\n# Remove highly correlated predictors (|r| &gt; 0.8)\ncor_matrix &lt;- cor(select(data_train_processed, where(is.numeric)))\nhigh_corr &lt;- names(which(apply(cor_matrix, 2, function(x) any(abs(x) &gt; 0.8 & abs(x) &lt; 1))))\n\n#data_train_processed &lt;- data_train_processed %&gt;% select(-all_of(high_corr))\ndata_train_processed &lt;- data_train_processed %&gt;% select(-c(\"weight\", \"waist\"))\n\n# Dummy encode categorical variables\ndata_train_processed &lt;- dummy_cols(data_train_processed,\n                                   select_columns = c(\"location\", \"gender\", \"frame\"),\n                                   remove_selected_columns = TRUE)\n\n# Separate outcome and predictors\ny_train &lt;- data_train_processed$BMI\nx_train &lt;- data_train_processed %&gt;% select(-BMI)\n\n# Scale predictors\nx_train_scaled &lt;- scale(x_train)\ntrain_means &lt;- attr(x_train_scaled, \"scaled:center\")\ntrain_sds &lt;- attr(x_train_scaled, \"scaled:scale\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Demo: a predictive modelling case study (base R approach)</span>"
    ]
  },
  {
    "objectID": "case-study-base-r.html#prepare-test-data-with-same-processing",
    "href": "case-study-base-r.html#prepare-test-data-with-same-processing",
    "title": "2  Demo: a predictive modelling case study (base R approach)",
    "section": "2.4 Prepare Test Data with Same Processing",
    "text": "2.4 Prepare Test Data with Same Processing\n\n# ---- Process Test Data ----\ndata_test_processed &lt;- data_test %&gt;%\n  mutate(\n    height = round(height * inch2m, 2),\n    weight = round(weight * pound2kg, 2),\n    glu = log(stab.glu)\n  ) %&gt;%\n  select(-stab.glu, -id)\n\n\n# Dummy encode categorical variables\ndata_test_processed &lt;- dummy_cols(data_test_processed,\n                                  select_columns = c(\"location\", \"gender\", \"frame\"),\n                                  remove_selected_columns = TRUE)\n\n# Remove same columns as in training\ndata_test_processed &lt;- data_test_processed %&gt;%\n  select(colnames(data_train_processed))\n\n# Ensure same column order as training\nx_test &lt;- data_test_processed %&gt;% select(-BMI)\nx_test &lt;- x_test[, colnames(x_train)]\n\n# Apply training set scaling\nx_test_scaled &lt;- scale(x_test, center = train_means, scale = train_sds)\ny_test &lt;- data_test_processed$BMI",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Demo: a predictive modelling case study (base R approach)</span>"
    ]
  },
  {
    "objectID": "case-study-base-r.html#lasso-regression-with-cross-validation",
    "href": "case-study-base-r.html#lasso-regression-with-cross-validation",
    "title": "2  Demo: a predictive modelling case study (base R approach)",
    "section": "2.5 Lasso Regression with Cross-Validation",
    "text": "2.5 Lasso Regression with Cross-Validation\n\n# Fit Lasso regression with 10-fold CV\nset.seed(123)\ncv_model &lt;- cv.glmnet(x_train_scaled, y_train, alpha = 1, standardize = FALSE)\n\n# Plot cross-validation error\nplot(cv_model)\n\n\n\n\n\n\n\n# Best lambda value\nbest_lambda &lt;- cv_model$lambda.min\ncat(\"Best lambda:\", best_lambda)\n\nBest lambda: 0.03330154",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Demo: a predictive modelling case study (base R approach)</span>"
    ]
  },
  {
    "objectID": "case-study-base-r.html#evaluate-model-on-test-data",
    "href": "case-study-base-r.html#evaluate-model-on-test-data",
    "title": "2  Demo: a predictive modelling case study (base R approach)",
    "section": "2.6 Evaluate Model on Test Data",
    "text": "2.6 Evaluate Model on Test Data\n\n# Predict\npred_test &lt;- predict(cv_model, s = best_lambda, newx = x_test_scaled)\n\n# RMSE\nrmse &lt;- sqrt(mean((pred_test - y_test)^2))\ncat(\"RMSE on test data:\", rmse)\n\nRMSE on test data: 2.729465\n\n# Correlation\ncor(pred_test, y_test)\n\n        [,1]\ns1 0.8872908\n\n# Scatter plot: Predicted vs Actual\nplot(y_test, pred_test,\n     xlab = \"Actual BMI\", ylab = \"Predicted BMI\", pch = 19,\n     main = \"Predicted vs Actual BMI\")\nabline(0, 1, col = \"red\", lwd = 2)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Demo: a predictive modelling case study (base R approach)</span>"
    ]
  },
  {
    "objectID": "case-study-base-r.html#variable-importance-plot",
    "href": "case-study-base-r.html#variable-importance-plot",
    "title": "2  Demo: a predictive modelling case study (base R approach)",
    "section": "2.7 Variable Importance Plot",
    "text": "2.7 Variable Importance Plot\n\n# Extract coefficients\ncoef_matrix &lt;- coef(cv_model, s = best_lambda)\ncoef_df &lt;- as.data.frame(as.matrix(coef_matrix))\ncoef_df$feature &lt;- rownames(coef_df)\ncolnames(coef_df)[1] &lt;- \"coefficient\"\n\n# Filter out intercept and zero coefficients\ncoef_df &lt;- coef_df %&gt;%\n  filter(feature != \"(Intercept)\", coefficient != 0) %&gt;%\n  mutate(abs_coef = abs(coefficient)) %&gt;%\n  arrange(desc(abs_coef))\n\n# Plot\nggplot(coef_df, aes(x = reorder(feature, abs_coef), y = abs_coef)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Variable Importance (Lasso Coefficients)\",\n       x = \"Feature\", y = \"Absolute Coefficient\") +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Demo: a predictive modelling case study (base R approach)</span>"
    ]
  },
  {
    "objectID": "case-study-tidymodels.html",
    "href": "case-study-tidymodels.html",
    "title": "3  Demo: a predictive modelling case study (tidymodels)",
    "section": "",
    "text": "3.1 Load data and perform EDA\nLet’s now use tidymodels framework to to build a predictive model for BMI using our diabetes data set. We will use:\n# load libraries\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ggcorrplot)\nlibrary(reshape2)\nlibrary(vip)\n\n# import raw data\ninput_diabetes &lt;- read_csv(\"data/data-diabetes.csv\")\n\n# create BMI variable\nconv_factor &lt;- 703 # conversion factor to calculate BMI from inches and pounds BMI = weight (lb) / [height (in)]2 x 703\ndata_diabetes &lt;- input_diabetes %&gt;%\n  mutate(BMI = weight / height^2 * 703, BMI = round(BMI, 2)) %&gt;%\n  relocate(BMI, .after = id)\n\n# preview data\nglimpse(data_diabetes)\n## Rows: 403\n## Columns: 20\n## $ id       &lt;dbl&gt; 1000, 1001, 1002, 1003, 1005, 1008, 1011, 1015, 1016, 1022, 1…\n## $ BMI      &lt;dbl&gt; 22.13, 37.42, 48.37, 18.64, 27.82, 26.50, 28.20, 34.33, 24.51…\n## $ chol     &lt;dbl&gt; 203, 165, 228, 78, 249, 248, 195, 227, 177, 263, 242, 215, 23…\n## $ stab.glu &lt;dbl&gt; 82, 97, 92, 93, 90, 94, 92, 75, 87, 89, 82, 128, 75, 79, 76, …\n## $ hdl      &lt;dbl&gt; 56, 24, 37, 12, 28, 69, 41, 44, 49, 40, 54, 34, 36, 46, 30, 4…\n## $ ratio    &lt;dbl&gt; 3.6, 6.9, 6.2, 6.5, 8.9, 3.6, 4.8, 5.2, 3.6, 6.6, 4.5, 6.3, 6…\n## $ glyhb    &lt;dbl&gt; 4.31, 4.44, 4.64, 4.63, 7.72, 4.81, 4.84, 3.94, 4.84, 5.78, 4…\n## $ location &lt;chr&gt; \"Buckingham\", \"Buckingham\", \"Buckingham\", \"Buckingham\", \"Buck…\n## $ age      &lt;dbl&gt; 46, 29, 58, 67, 64, 34, 30, 37, 45, 55, 60, 38, 27, 40, 36, 3…\n## $ gender   &lt;chr&gt; \"female\", \"female\", \"female\", \"male\", \"male\", \"male\", \"male\",…\n## $ height   &lt;dbl&gt; 62, 64, 61, 67, 68, 71, 69, 59, 69, 63, 65, 58, 60, 59, 69, 6…\n## $ weight   &lt;dbl&gt; 121, 218, 256, 119, 183, 190, 191, 170, 166, 202, 156, 195, 1…\n## $ frame    &lt;chr&gt; \"medium\", \"large\", \"large\", \"large\", \"medium\", \"large\", \"medi…\n## $ bp.1s    &lt;dbl&gt; 118, 112, 190, 110, 138, 132, 161, NA, 160, 108, 130, 102, 13…\n## $ bp.1d    &lt;dbl&gt; 59, 68, 92, 50, 80, 86, 112, NA, 80, 72, 90, 68, 80, NA, 66, …\n## $ bp.2s    &lt;dbl&gt; NA, NA, 185, NA, NA, NA, 161, NA, 128, NA, 130, NA, NA, NA, N…\n## $ bp.2d    &lt;dbl&gt; NA, NA, 92, NA, NA, NA, 112, NA, 86, NA, 90, NA, NA, NA, NA, …\n## $ waist    &lt;dbl&gt; 29, 46, 49, 33, 44, 36, 46, 34, 34, 45, 39, 42, 35, 37, 36, 3…\n## $ hip      &lt;dbl&gt; 38, 48, 57, 38, 41, 42, 49, 39, 40, 50, 45, 50, 41, 43, 40, 4…\n## $ time.ppn &lt;dbl&gt; 720, 360, 180, 480, 300, 195, 720, 1020, 300, 240, 300, 90, 7…\n\n# run basic EDA\n# note: we have seen descriptive statistics and plots during EDA session \n# note: so here we only look at missing data and correlation\n\n# calculate number of missing data per variable\ndata_na &lt;- data_diabetes %&gt;% \n  summarise(across(everything(), ~ sum(is.na(.)))) \n\n# make a table with counts sorted from highest to lowest\ndata_na_long &lt;- data_na %&gt;%\n  pivot_longer(-id, names_to = \"variable\", values_to = \"count\") %&gt;%\n  arrange(desc(count)) \n\n# make a column plot to visualize the counts\ndata_na_long %&gt;%\n  ggplot(aes(x = variable, y = count)) + \n  geom_col(fill = \"blue4\") + \n  xlab(\"\") + \n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\n# calculate correlation between numeric variables\ndata_cor &lt;- data_diabetes %&gt;% \n  dplyr::select(-id) %&gt;% \n  dplyr::select(where(is.numeric)) %&gt;%\n  cor(use = \"pairwise.complete.obs\")\n\n# visualize correlation via heatmap\nggcorrplot(data_cor, hc.order = TRUE, lab = FALSE)\n\n# based on the number of missing data, let's delete bp.2s, bp.2d\n# and use complete-cases analysis \ndata_diabetes_narm &lt;- data_diabetes %&gt;%\n  dplyr::select(-bp.2s, -bp.2d) %&gt;%\n  na.omit()\n\n\n\n\nNumber of missing data per variable, shows that bp.2d and bp.2s have more than 50% missing entries\n\n\n\n\n\n\n\nHeatmap visualizing Pearson correlation coefficient between numerical variables",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Demo: a predictive modelling case study (tidymodels)</span>"
    ]
  },
  {
    "objectID": "case-study-tidymodels.html#split-data",
    "href": "case-study-tidymodels.html#split-data",
    "title": "3  Demo: a predictive modelling case study (tidymodels)",
    "section": "3.2 Split data",
    "text": "3.2 Split data\n\n# use tidymodels framework to fit Lasso regression model for predicting BMI\n# using repeated cross-validation to tune lambda value in L1 penalty term\n\n# select random seed value\nmyseed &lt;- 123\n\n# split data into non-test (other) and test (80% s)\nset.seed(myseed)\ndata_split &lt;- initial_split(data_diabetes_narm, strata = BMI, prop = 0.8) # holds splitting info\ndata_other &lt;- data_split %&gt;% training() # creates non-test set (function is called training but it refers to non-test part)\ndata_test &lt;- data_split %&gt;% testing() # creates test set\n\n# prepare repeated cross-validation splits with 5 folds repeated 3 times\nset.seed(myseed)\ndata_folds &lt;- vfold_cv(data_other,\n                       v = 5, \n                       repeats = 3,\n                       strata = BMI)\n\n# check the split\ndim(data_diabetes)\n## [1] 403  20\ndim(data_other)\n## [1] 291  18\ndim(data_test)\n## [1] 75 18\n\n# check BMI distributions in data splits\npar(mfrow=c(3,1))\nhist(data_diabetes$BMI, xlab = \"\", main = \"BMI: all\", 50)\nhist(data_other$BMI, xlab = \"\", main = \"BMI: non-test\", 50)\nhist(data_test$BMI, xlab = \"\", main = \"BMI: test\", 50)\n\n\n\n\nDistribution of BMI values given all data and spits into non-test and test",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Demo: a predictive modelling case study (tidymodels)</span>"
    ]
  },
  {
    "objectID": "case-study-tidymodels.html#feature-engineering",
    "href": "case-study-tidymodels.html#feature-engineering",
    "title": "3  Demo: a predictive modelling case study (tidymodels)",
    "section": "3.3 Feature engineering",
    "text": "3.3 Feature engineering\n\n# create data recipe (feature engineering)\n\ninch2m &lt;- 2.54/100\npound2kg &lt;- 0.45\n\ndata_recipe &lt;- recipe(BMI ~ ., data = data_other) %&gt;%\n  update_role(id, new_role = \"sampleID\") %&gt;%\n  step_mutate(height = height * inch2m, height = round(height, 2)) %&gt;% # convert height to meters\n  step_mutate(weight = weight * pound2kg, weight = round(weight, 2)) %&gt;% # convert weight to kg\n  step_rename(glu = stab.glu) %&gt;% # rename stab.glu to glu\n  step_log(glu) %&gt;%  #ln transform glucose\n  step_zv(all_numeric()) %&gt;% # removes variables that are highly sparse and unbalanced (if found)\n  step_corr(all_numeric(), -all_outcomes(), -has_role(\"sampleID\"), threshold = 0.8) %&gt;% # removes variables with large absolute correlations with other variables (if found)\n  step_dummy(location, gender, frame) %&gt;% # convert categorical variables to dummy variables\n  step_normalize(all_numeric(), -all_outcomes(), -has_role(\"sampleID\"), skip = FALSE) \n  \n  # you can implement more steps: see https://recipes.tidymodels.org/reference/index.html\n\n# print recipe\ndata_recipe\n\n# check if recipe is doing what it is supposed to do\n# i.e. bake the data\ndata_other_prep &lt;- data_recipe %&gt;%\n  prep() %&gt;%\n  bake(new_data = NULL)\n\n## bake test data\ndata_test_prep &lt;- data_recipe %&gt;%\n  prep() %&gt;%\n  bake(new_data = data_test)\n\n# preview baked data\nprint(head(data_other_prep))\n## # A tibble: 6 × 17\n##      id   chol    glu    hdl  ratio  glyhb    age  height  bp.1s  bp.1d    hip\n##   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n## 1  1045 -0.319 -0.539 -0.830  0.486 -0.172 -0.645 -0.474  -1.16  -0.552 -1.66 \n## 2  1271  0.449 -1.09  -0.324  0.320 -0.458 -1.39  -1.29   -1.59  -1.00  -0.914\n## 3  1277 -0.657 -0.572  2.32  -1.45  -0.642 -0.337  1.56    0.286  2.15  -1.29 \n## 4  1303 -0.590 -0.410 -0.436 -0.178 -0.518  0.341  0.545  -0.309  0.500 -1.47 \n## 5  1309 -0.206 -0.347  0.687 -0.730 -0.860 -1.32   0.0357 -0.735 -0.402 -1.66 \n## 6  1315 -0.793 -0.572  0.350 -0.841  0.225  0.649  1.26   -0.565 -1.45  -1.29 \n## # ℹ 6 more variables: time.ppn &lt;dbl&gt;, BMI &lt;dbl&gt;, location_Louisa &lt;dbl&gt;,\n## #   gender_male &lt;dbl&gt;, frame_medium &lt;dbl&gt;, frame_small &lt;dbl&gt;",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Demo: a predictive modelling case study (tidymodels)</span>"
    ]
  },
  {
    "objectID": "case-study-tidymodels.html#lasso-regression",
    "href": "case-study-tidymodels.html#lasso-regression",
    "title": "3  Demo: a predictive modelling case study (tidymodels)",
    "section": "3.4 Lasso regression",
    "text": "3.4 Lasso regression\n\n# define model\nmodel &lt;- linear_reg(penalty = tune(), mixture = 1) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"regression\")\n\n# create workflow with data recipe and model \nwf &lt;- workflow() %&gt;%\n  add_model(model) %&gt;%\n  add_recipe(data_recipe)\n\n# define parameters range for tuning\ngrid_lambda &lt;- grid_regular(penalty(), levels = 25)\n\n# tune lambda\nmodel_tune &lt;- wf %&gt;%\n  tune_grid(resamples = data_folds, \n            grid = grid_lambda)\n\n# show metrics average across folds\nmodel_tune  %&gt;%\n  collect_metrics(summarize = TRUE)\n## # A tibble: 50 × 7\n##     penalty .metric .estimator  mean     n std_err .config              \n##       &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n##  1 1   e-10 rmse    standard   2.48     15  0.142  Preprocessor1_Model01\n##  2 1   e-10 rsq     standard   0.851    15  0.0143 Preprocessor1_Model01\n##  3 2.61e-10 rmse    standard   2.48     15  0.142  Preprocessor1_Model02\n##  4 2.61e-10 rsq     standard   0.851    15  0.0143 Preprocessor1_Model02\n##  5 6.81e-10 rmse    standard   2.48     15  0.142  Preprocessor1_Model03\n##  6 6.81e-10 rsq     standard   0.851    15  0.0143 Preprocessor1_Model03\n##  7 1.78e- 9 rmse    standard   2.48     15  0.142  Preprocessor1_Model04\n##  8 1.78e- 9 rsq     standard   0.851    15  0.0143 Preprocessor1_Model04\n##  9 4.64e- 9 rmse    standard   2.48     15  0.142  Preprocessor1_Model05\n## 10 4.64e- 9 rsq     standard   0.851    15  0.0143 Preprocessor1_Model05\n## # ℹ 40 more rows\n\n# plot k-folds results across lambda range\nmodel_tune %&gt;%\n  collect_metrics() %&gt;% \n  dplyr::filter(.metric == \"rmse\") %&gt;% \n  ggplot(aes(penalty, mean, color = .metric)) +\n  geom_errorbar(aes( ymin = mean - std_err, ymax = mean + std_err), alpha = 0.5) +\n  scale_x_log10() + \n  geom_line(linewidth = 1.5) +\n  theme_bw() +\n  theme(legend.position = \"none\") +\n  scale_color_brewer(palette = \"Set1\")\n  \n# best lambda value (min. RMSE)\nmodel_best &lt;- model_tune %&gt;%\n  select_best(metric = \"rmse\")\n\nprint(model_best)\n## # A tibble: 1 × 2\n##   penalty .config              \n##     &lt;dbl&gt; &lt;chr&gt;                \n## 1  0.0562 Preprocessor1_Model22\n\n# finalize workflow with tuned model\nwf_final &lt;- wf %&gt;%\n  finalize_workflow(model_best)\n\n# last fit \nfit_final &lt;- wf_final %&gt;%\n  last_fit(split = data_split)\n\n# final predictions\ny_test_pred &lt;- fit_final %&gt;% collect_predictions() # predicted BMI\n\n# final predictions: performance on test (unseen data)\nfit_final %&gt;% collect_metrics() \n## # A tibble: 2 × 4\n##   .metric .estimator .estimate .config             \n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n## 1 rmse    standard       2.79  Preprocessor1_Model1\n## 2 rsq     standard       0.857 Preprocessor1_Model1\n\n# plot predictions vs. actual for test data\nplot(data_test$BMI, y_test_pred$.pred, xlab=\"BMI (actual)\", ylab = \"BMI (predicted)\", las = 1, pch = 19)\n\n# correlation between predicted and actual BMI values for test data\ncor(data_test$BMI, y_test_pred$.pred)\n## [1] 0.9256857\n\n# re-fit model on all non-test data\nmodel_final &lt;- wf_final %&gt;%\n  fit(data_other) \n\n# show final model\ntidy(model_final)\n## # A tibble: 16 × 3\n##    term            estimate penalty\n##    &lt;chr&gt;              &lt;dbl&gt;   &lt;dbl&gt;\n##  1 (Intercept)      28.7     0.0562\n##  2 chol              0       0.0562\n##  3 glu               0.0229  0.0562\n##  4 hdl               0       0.0562\n##  5 ratio             0.335   0.0562\n##  6 glyhb            -0.0512  0.0562\n##  7 age              -0.257   0.0562\n##  8 height           -1.86    0.0562\n##  9 bp.1s            -0.294   0.0562\n## 10 bp.1d             0.203   0.0562\n## 11 hip               5.60    0.0562\n## 12 time.ppn          0       0.0562\n## 13 location_Louisa  -0.160   0.0562\n## 14 gender_male       1.07    0.0562\n## 15 frame_medium     -0.320   0.0562\n## 16 frame_small      -0.530   0.0562\n\n# plot variables ordered by importance (highest abs(coeff))\nmodel_final %&gt;%\n  extract_fit_parsnip() %&gt;%\n  vip(geom = \"point\") + \n  theme_bw()\n\n\n\n\nMean RMSE plus/minus standard error across repeated cross-validation folds as a function of lambda values\n\n\n\n\n\n\n\nPredicted BMI values against actual BMI values using final model for predicting test (unseen) data\n\n\n\n\n\n\n\nTop feature of importance, here measured as the features with highest absolute value of the Lasso regression coefficients from the final tuned model",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Demo: a predictive modelling case study (tidymodels)</span>"
    ]
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Answers to exercises\nRunning the code with the same seed value should yield \\(\\lambda = 1\\) and \\(\\alpha = 0.222\\). Well done, now you know how to fit models with more than one parameter. For algorithms with more parameters we follow the same principles. Above we have used grid_regular() to generate search space for \\(\\lambda\\) and \\(\\alpha\\) as we know from the regularized regression that these have to be between 0 and 1. For other algorithms, the search space may be harder to define. One can then for instance start with a random grid and later tune the grid based on the initial training. There are also some other helpful packages under tune package to experiment with, see https://dials.tidymodels.org/reference/index.html under Grid Creation.\nWhat have we learned from the Elastic Net model? We can see from the \\(\\beta\\) coefficients that after excluding all predictors related closely to BMI such as “weight” and “waist”, the most important variables to predict BMI are “hdl”, “gender” and “height”. Overall, the model holds some predictive power, but with quite high \\(RMSE = 6.79\\) and low \\(R^2 = 0.141\\) we should not expect of being predict BMI for new data very accurately.\nRunning the code with the same seed value should yield \\(SN = 0.207\\) and \\(PPV = 0.6\\). In our imaginary scenario, where we cannot measure obesity directly, we could use any model that can diagnose obesity, even if only on average we would be able to diagnose correctly every 5th obese patient as obese. However, with precision of 0.6 the model is not great at avoiding false positives. As the medications have severe side effects in non-obese people by using by using this model for diagnosis and treatment, we would end up with more unwell people that without using the model.",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "exercises.html#answers-to-exercises",
    "href": "exercises.html#answers-to-exercises",
    "title": "Exercises",
    "section": "",
    "text": "Solution. Exercise 2\n\n#| label: load-data\n#| eval: false\n#| warning: false\n#| message: false\n#| code-fold: false\n#| collapse: true\n#| fig-show: hold\n#| fig-cap-location: margin\n\n# load libraries\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ggcorrplot)\nlibrary(reshape2)\nlibrary(vip)\n\n# import raw data\ninput_diabetes &lt;- read_csv(\"data/data-diabetes.csv\")\n\n# create BMI variable\nconv_factor &lt;- 703 # conversion factor to calculate BMI from inches and pounds BMI = weight (lb) / [height (in)]2 x 703\ndata_diabetes &lt;- input_diabetes %&gt;%\n  mutate(BMI = weight / height^2 * 703, BMI = round(BMI, 2)) %&gt;%\n  relocate(BMI, .after = id)\n\n# preview data\nglimpse(data_diabetes)\n\n# run basic EDA\n# note: we have seen descriptive statistics and plots during EDA session \n# note: so here we only look at missing data and correlation\n\n# calculate number of missing data per variable\ndata_na &lt;- data_diabetes %&gt;% \n  summarise(across(everything(), ~ sum(is.na(.)))) \n\n# basd on the number of missing data, let's delete bp.2s, bp.2d\n# and use complete-cases analysis \ndata_diabetes_narm &lt;- data_diabetes %&gt;%\n  dplyr::select(-bp.2s, -bp.2d) %&gt;%\n  na.omit()\n\n# use tiymodels framework to fit Lasso regression model for predicting BMI\n# using repeated cross-validation to tune lambda value in L1 penalty term\n\n# select random seed value\nmyseed &lt;- 123\n\n# split data into non-test (other) and test (80% s)\nset.seed(myseed)\ndata_split &lt;- initial_split(data_diabetes_narm, strata = BMI, prop = 0.8) # holds splitting info\ndata_other &lt;- data_split %&gt;% training() # creates non-test set (function is called training but it refers to non-test part)\ndata_test &lt;- data_split %&gt;% testing() # creates test set\n\n# prepare repeated cross-validation splits with 5 folds repeated 3 times\nset.seed(myseed)\ndata_folds &lt;- vfold_cv(data_other,\n                       v = 5, \n                       repeats = 3,\n                       strata = BMI)\n\n# check the split\ndim(data_diabetes)\ndim(data_other)\ndim(data_test)\n\n# check BMI distributions in data splits\npar(mfrow=c(3,1))\nhist(data_diabetes$BMI, xlab = \"\", main = \"BMI: all\", 50)\nhist(data_other$BMI, xlab = \"\", main = \"BMI: non-test\", 50)\nhist(data_test$BMI, xlab = \"\", main = \"BMI: test\", 50)\n\n# create data recipe (feature engineering)\ninch2m &lt;- 2.54/100\npound2kg &lt;- 0.45\ndata_recipe &lt;- recipe(BMI ~ ., data = data_other) %&gt;%\n  update_role(id, new_role = \"sampleID\") %&gt;%\n  step_rm(weight, waist, hip, frame) %&gt;%\n  step_mutate(height = height * inch2m, height = round(height, 2)) %&gt;% # convert height to meters\n  #step_mutate(weight = weight * pound2kg, weight = round(weight, 2)) %&gt;% # convert weight to kg\n  step_rename(glu = stab.glu) %&gt;% # rename stab.glu to glu\n  step_log(glu) %&gt;%  #ln transform glucose\n  step_zv(all_numeric()) %&gt;% # removes variables that are highly sparse and unbalanced (if found)\n  step_corr(all_numeric(), -all_outcomes(), -has_role(\"sampleID\"), threshold = 0.8) %&gt;% # removes variables with large absolute correlations with other variables (if found)\n  step_dummy(location, gender) %&gt;% # convert categorical variables to dummy variables\n  step_normalize(all_numeric(), -all_outcomes(), -has_role(\"sampleID\"), skip = FALSE) \n  # you can implement more steps: see https://recipes.tidymodels.org/reference/index.html\n\n# print recipe\ndata_recipe\n\n# check if recipe is doing what it is supposed to do\n# i.e. bake the data\ndata_other_prep &lt;- data_recipe %&gt;%\n  prep() %&gt;%\n  bake(new_data = NULL)\n\n## bake test data\ndata_test_prep &lt;- data_recipe %&gt;%\n  prep() %&gt;%\n  bake(new_data = data_test)\n\n# preview baked data\nprint(head(data_other_prep))\n\n# Elastic Net\n# define model\nmodel &lt;- linear_reg(penalty = tune(), mixture = tune()) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"regression\")\n\n# create workflow with data recipe and model \nwf &lt;- workflow() %&gt;%\n  add_model(model) %&gt;%\n  add_recipe(data_recipe)\n\n# define parameters range for tuning\n# let's check fewer lambda values\n# as now we also need to add alpha values and look at the combinations of both\ngrid_param &lt;- grid_regular(penalty(), mixture(), levels = 10)\ngrid_param\n\n# tune lambda\nmodel_tune &lt;- wf %&gt;%\n  tune_grid(resamples = data_folds, \n            grid = grid_param)\n\n# show metrics average across folds\nmodel_tune  %&gt;%\n  collect_metrics(summarize = TRUE)\n\n# best lambda value (min. RMSE)\nmodel_best &lt;- model_tune %&gt;%\n  select_best(metric = \"rmse\")\nprint(model_best)\n\n# finalize workflow with tuned model\nwf_final &lt;- wf %&gt;%\n  finalize_workflow(model_best)\n\n# last fit \nfit_final &lt;- wf_final %&gt;%\n  last_fit(split = data_split)\n\n# final predictions\ny_test_pred &lt;- fit_final %&gt;% collect_predictions() # predicted BMI\n\n# final predictions: performance on test (unseen data)\nfit_final %&gt;% collect_metrics() \n\n# plot predictions vs. actual for test data\nplot(data_test$BMI, y_test_pred$.pred, xlab=\"BMI (actual)\", ylab = \"BMI (predicted)\", las = 1, pch = 19)\n\n# correlation between predicted and actual BMI values for test data\ncor(data_test$BMI, y_test_pred$.pred)\n\n# re-fit model on all non-test data\nmodel_final &lt;- wf_final %&gt;%\n  fit(data_other) \n\n# show final model\ntidy(model_final)\n\n# plot variables ordered by importance (highest abs(coeff))\nmodel_final %&gt;%\n  extract_fit_parsnip() %&gt;%\n  vip(geom = \"point\") + \n  theme_bw()\n\n\n\nSolution. Exercise 3\n\n#| label: load-data\n#| eval: false\n#| warning: false\n#| message: false\n#| code-fold: false\n#| collapse: true\n#| fig-show: hold\n#| fig-cap-location: margin\n\n# load libraries\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ggcorrplot)\nlibrary(reshape2)\nlibrary(vip)\nlibrary(kknn)\n\n# import raw data\ninput_diabetes &lt;- read_csv(\"data/data-diabetes.csv\")\n\n# create BMI variable\nconv_factor &lt;- 703 # conversion factor to calculate BMI from inches and pounds BMI = weight (lb) / [height (in)]2 x 703\ndata_diabetes &lt;- input_diabetes %&gt;%\n  mutate(BMI = weight / height^2 * 703, BMI = round(BMI, 2)) %&gt;%\n  relocate(BMI, .after = id) %&gt;%\n  mutate(obese = ifelse(BMI &gt; 30, \"Yes\", \"No\")) %&gt;% # create obese categorical variable\n  mutate(obese = factor(obese, levels = c(\"Yes\", \"No\"))) %&gt;%\n  relocate(obese, .after = BMI) \n  \n# preview data\nglimpse(data_diabetes)\n\n# run basic EDA\n# note: we have seen descriptive statistics and plots during EDA session \n# note: so here we only look at missing data and correlation\n\n# calculate number of missing data per variable\ndata_na &lt;- data_diabetes %&gt;% \n  summarise(across(everything(), ~ sum(is.na(.)))) \n\n# basd on the number of missing data, let's delete bp.2s, bp.2d\n# and use complete-cases analysis \ndata_diabetes_narm &lt;- data_diabetes %&gt;%\n  dplyr::select(-bp.2s, -bp.2d) %&gt;%\n  na.omit()\n\n# use tiymodels framework to fit KNN to predict obesity (Yes/No)\n# using repeated cross-validation to tune k neighbours\n\n# select random seed value\nmyseed &lt;- 123\n\n# split data into non-test (other) and test (80% s)\nset.seed(myseed)\ndata_split &lt;- initial_split(data_diabetes_narm, strata = obese, prop = 0.8) # holds splitting info\ndata_other &lt;- data_split %&gt;% training() # creates non-test set (function is called training but it refers to non-test part)\ndata_test &lt;- data_split %&gt;% testing() # creates test set\n\n# prepare repeated cross-validation splits with 5 folds repeated 3 times\nset.seed(myseed)\ndata_folds &lt;- vfold_cv(data_other,\n                       v = 5, \n                       repeats = 3,\n                       strata = obese)\n\n# check the split\ndim(data_diabetes)\ndim(data_other)\ndim(data_test)\n\n# check obese counts in data splits\npar(mfrow=c(3,1))\nsummary(data_diabetes$obese)\nsummary(data_other$obese)\nsummary(data_test$obese)\n\n# create data recipe (feature engineering)\ninch2m &lt;- 2.54/100\npound2kg &lt;- 0.45\ndata_recipe &lt;- recipe(obese ~ ., data = data_other) %&gt;%\n  step_rm(BMI, weight, waist, frame, hip) %&gt;% # exclude BMI, weight, waist\n  update_role(id, new_role = \"sampleID\") %&gt;%\n  step_mutate(height = height * inch2m, height = round(height, 2)) %&gt;% # convert height to meters\n  #step_mutate(weight = weight * pound2kg, weight = round(weight, 2)) %&gt;% # convert weight to kg\n  step_rename(glu = stab.glu) %&gt;% # rename stab.glu to glu\n  step_log(glu) %&gt;%  #ln transform glucose\n  step_zv(all_numeric()) %&gt;% # removes variables that are highly sparse and unbalanced (if found)\n  step_corr(all_numeric(), -all_outcomes(), -has_role(\"sampleID\"), threshold = 0.8) %&gt;% # removes variables with large absolute correlations with other variables (if found)\n  step_dummy(location, gender) %&gt;% # convert categorical variables to dummy variables\n  step_normalize(all_numeric(), -all_outcomes(), -has_role(\"sampleID\"), skip = FALSE) \n  # you can implement more steps: see https://recipes.tidymodels.org/reference/index.html\n\n# print recipe\ndata_recipe\n\n# check if recipe is doing what it is supposed to do\n# i.e. bake the data\ndata_other_prep &lt;- data_recipe %&gt;%\n  prep() %&gt;%\n  bake(new_data = NULL)\n\n## bake test data\ndata_test_prep &lt;- data_recipe %&gt;%\n  prep() %&gt;%\n  bake(new_data = data_test)\n\n# preview baked data\nprint(head(data_other_prep))\n\n# KNN\n# create model\nmodel &lt;- nearest_neighbor(neighbors = tune(), weight_func = \"rectangular\") %&gt;%\n  set_engine(\"kknn\", scale = FALSE) %&gt;%\n  set_mode(\"classification\") \n\n# create workflow with data recipe and model \nwf &lt;- workflow() %&gt;%\n  add_model(model) %&gt;%\n  add_recipe(data_recipe)\n\n# define parameters range for tuning k, let's check even number from 1 to 51\ngrid_param &lt;- as_tibble(data.frame(neighbors = seq(1,51, 2)))\ngrid_param\n\n# tune model\nmodel_tune &lt;- wf %&gt;%\n  tune_grid(resamples = data_folds, \n            grid = grid_param)\n\n# show metrics average across folds\nmodel_tune  %&gt;%\n  collect_metrics(summarize = TRUE)\n\n# best parameter value, we can choose accuracy or roc_auc\nmodel_best &lt;- model_tune %&gt;% \n  select_best(metric = \"accuracy\")\nprint(model_best)\n\n# finalize workflow with tuned model\nwf_final &lt;- wf %&gt;%\n  finalize_workflow(model_best)\n\nwf_final\n\n# last fit \nfit_final &lt;- wf_final %&gt;%\n  last_fit(split = data_split)\n\n# final predictions\ny_test_pred &lt;- fit_final %&gt;% collect_predictions() # predicted BMI\n\n# final predictions: performance on test (unseen data)\nfit_final %&gt;% collect_metrics() \n\n# show confusion matrix for predictions\ny_actual &lt;- factor(data_test$obese, levels = c(\"Yes\", \"No\"))\ny_pred &lt;- factor(y_test_pred$.pred_class, levels = c(\"Yes\", \"No\"))\ntable(y_actual, y_pred, dnn = c(\"Actual\", \"Predicted\"))\n\n# we can calculate SN (recall) and precision using our confusion-matrix by following equations from the \"supervised learning\" lecture:\n\nSN &lt;- 6 / (6 + 23)\nPPV &lt;- 6 / (6 + 4)\n\n# or we use yardstick package to add these metrics to the default ones (accuracy and roc_auc)\nclass_metrics &lt;-  metric_set(accuracy, roc_auc, sens, precision)\nfit_final %&gt;% collect_predictions() %&gt;%\n  class_metrics(truth = obese, estimate = .pred_class, `.pred_Yes`)",
    "crumbs": [
      "Exercises"
    ]
  }
]